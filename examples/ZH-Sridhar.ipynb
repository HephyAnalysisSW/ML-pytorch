{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9736663f-41bd-4b08-954b-fd212fffbc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.24/06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import ROOT\n",
    "from Tools import syncer \n",
    "from Tools import user\n",
    "from Tools import helpers\n",
    "import itertools\n",
    "from PIL import Image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92336f08-671b-492c-8265-0dc5cbe7f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self,nfeatures,coefficient_names):\n",
    "        self.nfeatures         = nfeatures\n",
    "        self.coefficient_names = coefficient_names\n",
    "        self.combination_list=list(itertools.chain.from_iterable(itertools.combinations_with_replacement(self.coefficient_names, i) for i in np.arange(0,3)))\n",
    "        self.n_hat = {combination: self.make_NN() for combination in self.combination_list}\n",
    "        \n",
    "    def make_NN(self, hidden_layers  = [64, 64, 64, 64]):\n",
    "        '''\n",
    "        Creates the Neural Network Architecture\n",
    "        '''\n",
    "        model_nn = [torch.nn.BatchNorm1d(self.nfeatures), torch.nn.ReLU(), torch.nn.Linear(self.nfeatures, hidden_layers[0])]\n",
    "        for i_layer, layer in enumerate(hidden_layers):\n",
    "            model_nn.append(torch.nn.Linear(hidden_layers[i_layer], hidden_layers[i_layer+1] if i_layer+1<len(hidden_layers) else 1))\n",
    "            if i_layer+1<len(hidden_layers):\n",
    "                model_nn.append( torch.nn.ReLU() )\n",
    "        return torch.nn.Sequential(*model_nn)\n",
    "\n",
    "    def evaluate_NN(self, features):\n",
    "        '''Evaluate Neural Network: The zeroth dimension of features is the number of data points and and the first dimension\n",
    "        is the number of features(variables). Returns the output of the NNs of dimensions: (noutput,ndatapoints)\n",
    "        '''\n",
    "        noutputs=len(self.combination_list)\n",
    "        ndatapoints=features.shape[0]\n",
    "        \n",
    "        output=torch.zeros((noutputs,ndatapoints))\n",
    "        for i in range(noutputs):\n",
    "            x=self.n_hat[self.combination_list[i]](features)\n",
    "            if i==0:\n",
    "                output[i,:]=1\n",
    "            else:\n",
    "                output[i,:]=torch.flatten(x)            \n",
    "        return output\n",
    "\n",
    "    def predict_r_hat2(self, predictions,eft):\n",
    "        return torch.add( \n",
    "        torch.sum( torch.stack( [(1. + predictions[(c,)]*eft[c])**2 for c in coefficients ]), dim=0),\n",
    "        torch.sum( torch.stack( [torch.sum( torch.stack( [ predictions[(c_1,c_2)]*eft[c_2] for c_2 in coefficients[i_c_1:] ]), dim=0)**2 for i_c_1, c_1 in enumerate(coefficients) ] ), dim=0 ) )   \n",
    "\n",
    "    def save(self,fileName):\n",
    "        outfile = open(fileName,'wb')\n",
    "        pickle.dump(self, outfile)\n",
    "        outfile.close()\n",
    "        \n",
    "    @classmethod\n",
    "    def load(self, fileName):\n",
    "        infile = open(fileName,'rb')\n",
    "        print(fileName)\n",
    "        new_dict = pickle.load(infile)\n",
    "        infile.close()\n",
    "        return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f300bba0-ec54-4e0e-9b9e-4e12718c1653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weight_ratio(weights, **kwargs):\n",
    "    eft      = kwargs\n",
    "    result = torch.ones(len(weights[()])) \n",
    "    for combination in combinations:\n",
    "        if len(combination)==1:\n",
    "            result += eft[combination[0]]*weights[combination]/weights[()]\n",
    "        elif len(combination)==2:# add up without the factor 1/2 because off diagonals are only summed in upper triangle \n",
    "            result += (0.5 if len(set(combination))==1 else 1.)*eft[combination[0]]*eft[combination[1]]*weights[combination]/weights[()]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecb14fda-63c7-40fd-8558-e73b040ae190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functional\n",
    "def f_loss(r_NN, features, predictions, base_points):\n",
    "    loss = -0.5*weights[()].sum()\n",
    "    for i_base_point, base_point in enumerate(base_points):\n",
    "        fhat  = 1./(1. + r_NN.predict_r_hat2(predictions, base_point))\n",
    "        loss += ( torch.tensor(weights[()])*( -0.25 + base_point_weight_ratios[i_base_point]*fhat**2 + (1-fhat)**2 ) ).sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3ba614a-e60f-487b-99c0-3312429a1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_directory=\"14_7_2022\"\n",
    "nEvents=30000\n",
    "learning_rate = 1e-3\n",
    "device        = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_epoch       = 3000\n",
    "plot_every    = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4a711d-6147-429a-8b5e-5d37a53e2526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested 30000 events. Simulated 30000 events and 30000 survive pT_min cut of 0.\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "import ZH_Nakamura\n",
    "\n",
    "ZH_Nakamura.feature_names = ZH_Nakamura.feature_names[0:6] # restrict features\n",
    "features   = ZH_Nakamura.getEvents(nEvents)[:,0:6]\n",
    "feature_names  = ZH_Nakamura.feature_names\n",
    "plot_options   = ZH_Nakamura.plot_options\n",
    "plot_vars      = ZH_Nakamura.feature_names\n",
    "\n",
    "mask       = (features[:,feature_names.index('pT')]<900) & (features[:,feature_names.index('sqrt_s_hat')]<1800) \n",
    "features = features[mask]\n",
    "\n",
    "nfeatures = len(features[0]) \n",
    "weights    = ZH_Nakamura.getWeights(features, ZH_Nakamura.make_eft())\n",
    "\n",
    "train_sel, test_sel = list(map(list,torch.utils.data.random_split(range(len(features)), [len(features)//2,len(features)-len(features)//2] )))\n",
    "\n",
    "\n",
    "\n",
    "#pT=features[:,feature_names.index('pT')]\n",
    "\n",
    "coefficients   = ['cHW']\n",
    "combinations   =  [ (), ('cHW',), ('cHW', 'cHW')]\n",
    "combinations2   =  [ (), ('cHW',), ('cHW', 'cHW')]\n",
    "base_points = [ {'cHW':value} for value in [-1.5, -.8, -.4, -.2, .2, .4, .8, 1.5] ]\n",
    "\n",
    "#coefficients   = ['cHWtil']\n",
    "#combinations   =  [ (), ('cHWtil',), ('cHWtil', 'cHWtil')]\n",
    "#combinations2   =  [ (), ('cHWtil',), ('cHWtil', 'cHWtil')]\n",
    "#base_points = [ {'cHWtil':value} for value in [-1.5, -.8, -.4, -.2, .2, .4, .8, 1.5] ]\n",
    "\n",
    "#coefficients   =  ( 'cHW', 'cHWtil') \n",
    "#combinations   =  [(), ('cHW',), ('cHWtil',), ('cHW','cHW'), ('cHW','cHWtil'),('cHWtil','cHWtil')]\n",
    "#combinations2 = [(), ('cHW',), ('cHWtil',), ('cHW','cHW'), ('cHW','cHWtil'),('cHWtil','cHW'),('cHWtil','cHWtil')]\n",
    "#base_points = [ {'cHW':value1, 'cHWtil':value2} for value1 in [-1.5, -.8, .2, 0., .2, .8, 1.5]  for value2 in [-1.5, -.8, .2, 0, .2, .8, 1.5]]\n",
    "\n",
    "rows, columns = np.triu_indices(len(coefficients)+1)\n",
    "w=np.zeros((len(coefficients)+1,len(coefficients)+1,features.shape[0]))\n",
    "\n",
    "for i in range(0,len(combinations)):\n",
    "    w[rows[i],columns[i],:]=torch.from_numpy(weights[combinations[i]]).float().to(device)\n",
    "    if rows[i]!=columns[i]:\n",
    "        w[columns[i],rows[i],:]=w[rows[i],columns[i],:]\n",
    "\n",
    "features_train = torch.from_numpy(features[train_sel]).float().to(device)\n",
    "features_test = torch.from_numpy(features[test_sel]).float().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a51735a-584a-4025-8521-fcef189132bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = tuple(filter( lambda coeff: coeff in coefficients, list(coefficients))) \n",
    "combinations = tuple(filter( lambda comb: all([c in coefficients for c in comb]), combinations)) \n",
    "\n",
    "base_points    = list(map( lambda b:ZH_Nakamura.make_eft(**b), base_points ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "350f60f1-d0c7-40b8-a9f9-c7edcfe06ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_NN=NN(nfeatures,coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58893910-4759-43c7-ab89-0582a047c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_point_weight_ratios = list( map( lambda base_point: make_weight_ratio( weights, **base_point ), base_points ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd9a6e4-c7cc-4c1d-b0ef-794086eedb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(sum([list(model.parameters()) for model in r_NN.n_hat.values()],[]), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "tex = ROOT.TLatex()\n",
    "tex.SetNDC()\n",
    "tex.SetTextSize(0.04)\n",
    "hist_colors=['b','g', 'r', 'c', 'm','y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cdddd70-6982-4579-a182-0208a4f7224e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99 loss 0.25761127143509793\n",
      "epoch 199 loss 0.2557632368091234\n",
      "epoch 299 loss 0.25390197213709187\n",
      "epoch 399 loss 0.24935207543335114\n",
      "epoch 499 loss 0.24555656821154942\n",
      "epoch 599 loss 0.24275909454758793\n",
      "epoch 699 loss 0.24241562454684434\n",
      "epoch 799 loss 0.24218488165866983\n",
      "epoch 899 loss 0.24197569628651153\n",
      "epoch 999 loss 0.24090291757862575\n",
      "epoch 1099 loss 0.2405327751277187\n",
      "epoch 1199 loss 0.24028053818360157\n",
      "epoch 1299 loss 0.24014786219916062\n",
      "epoch 1399 loss 0.2399295174100702\n",
      "epoch 1499 loss 0.2397639316393787\n",
      "epoch 1599 loss 0.2395926073873346\n",
      "epoch 1699 loss 0.2394565622676511\n",
      "epoch 1799 loss 0.2393653526781297\n",
      "epoch 1899 loss 0.23928697536415522\n",
      "epoch 1999 loss 0.23921347612270247\n",
      "epoch 2099 loss 0.23910134199828587\n",
      "epoch 2199 loss 0.23900989816486312\n",
      "epoch 2299 loss 0.23896682138171815\n",
      "epoch 2399 loss 0.23894398226703822\n",
      "epoch 2499 loss 0.2388625186301182\n",
      "epoch 2599 loss 0.23882801341755538\n",
      "epoch 2699 loss 0.23879143672114395\n",
      "epoch 2799 loss 0.23876581615108275\n",
      "epoch 2899 loss 0.23876043807073044\n",
      "epoch 2999 loss 0.23874014330595636\n"
     ]
    }
   ],
   "source": [
    "for nn in r_NN.n_hat.values():\n",
    "    nn.train()\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    #print(\"epoch: \", epoch)\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    predictions = {combination:r_NN.n_hat[combination](features).squeeze() for combination in combinations}\n",
    "    \n",
    "    #rows, columns = np.triu_indices(len(coefficients)+1)\n",
    "    w_predicted=np.zeros((len(coefficients)+1, len(coefficients)+1, features.shape[0]))      \n",
    "    for i in range(1,len(combinations)):\n",
    "        w_predicted[rows[i],columns[i],:]=predictions[combinations[i]].squeeze().cpu().detach().numpy()\n",
    "        if rows[i]!=columns[i]:\n",
    "            w_predicted[columns[i],rows[i],:]=w_predicted[rows[i],columns[i],:]\n",
    "    \n",
    "    wp=np.zeros((len(coefficients),features.shape[0]))\n",
    "    for i in range(0,len(coefficients)):\n",
    "        wp[i,:]=2*np.sum(w_predicted[:,i+1,:],0)\n",
    "    \n",
    "    wpp=np.zeros((len(coefficients),len(coefficients),len(coefficients),features.shape[0]))\n",
    "    for i in range(0,len(coefficients)):\n",
    "        for l in range(0,len(coefficients)):\n",
    "            for k in range(0,len(coefficients)):\n",
    "                wpp[i,l,k,:]=2*w_predicted[i,l+1,:]*w_predicted[i,k+1,:]\n",
    "    \n",
    "    wpp=np.sum(wpp,0)\n",
    "               \n",
    "    # Compute and print loss.\n",
    "    loss = f_loss(r_NN,features, predictions,base_points)\n",
    "    losses.append(loss.item())\n",
    "    if epoch % 100 == 99:\n",
    "        print(\"epoch\", epoch, \"loss\",  loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "\n",
    "    if (epoch % plot_every)==0:\n",
    "        with torch.no_grad():\n",
    "                \n",
    "            #print(\"epoch\", epoch, \"loss inside loop\",  loss_train.item())\n",
    "            #print(\"epoch\", epoch, \"test loss\",  loss_test.item())\n",
    "            \n",
    "            for var in ['pT']:\n",
    "                binning     = plot_options[var]['binning']\n",
    "                np_binning= np.linspace(binning[1], binning[2], 1+binning[0])\n",
    "                \n",
    "                # Linear Terms\n",
    "                \n",
    "                \n",
    "\n",
    "                for i in range(1,len(coefficients)+1):\n",
    "                    hist_truth_0, bins  = np.histogram(features[:,feature_names.index(var)], np_binning, weights=w[0,0,:])\n",
    "                    plt.step(bins[1:], hist_truth_0, label='yield', linestyle=('solid'), color='k')\n",
    "                    hist_truth, bins = np.histogram(features[:,feature_names.index(var)], np_binning, weights=w[0,i,:])\n",
    "                    #plt.step(bins[1:], hist_truth/hist_truth_0, label='truth', linestyle=('dashed'), color=hist_colors[i])\n",
    "                    plt.step(bins[1:], hist_truth, label='truth', linestyle=('dashed'), color=hist_colors[i])\n",
    "                    hist_predicted, bins = np.histogram(features[:,feature_names.index(var)], bins, weights=w[0,0,:]*wp[i-1,:])\n",
    "                    #plt.step(bins[1:], hist_predicted/hist_truth_0, label='truth', linestyle=('solid'), color=hist_colors[i])\n",
    "                    plt.step(bins[1:], hist_predicted, label='truth', linestyle=('solid'), color=hist_colors[i])\n",
    "                    plt.legend(['Yield ', 'Truth '+ coefficients[i-1], 'Predicted '+ coefficients[i-1]])\n",
    "                    plt.savefig(os.path.join(plot_directory, \"_epoch_%05i_%s__\"%(epoch, var)+coefficients[i-1] + \".png\" ))\n",
    "                    plt.close()\n",
    "                \n",
    "                \n",
    "                a = np.arange(len(coefficients)**2).reshape(len(coefficients),len(coefficients))\n",
    "                for l in range(0,len(coefficients)):\n",
    "                    for k in range(0,len(coefficients)):\n",
    "                        label=combinations2[a[l,k]+len(coefficients)+1][0]+ '_' + combinations2[a[l,k]+len(coefficients)+1][1]\n",
    "                        hist_truth_0, bins  = np.histogram(features[:,feature_names.index(var)], np_binning, weights=w[0,0,:])\n",
    "                        plt.step(bins[1:], hist_truth_0, label='yield', linestyle=('solid'), color='k')\n",
    "                        hist_truth, bins  = np.histogram(features[:,feature_names.index(var)], np_binning, weights=w[l+1,k+1,:])\n",
    "                        #plt.step(bins[1:],hist_truth/hist_truth_0, label='truth', linestyle=('dashed'), color=hist_colors[a[l,k]+len(coefficients)])\n",
    "                        plt.step(bins[1:],hist_truth, label='truth', linestyle=('dashed'), color=hist_colors[a[l,k]+len(coefficients)])\n",
    "                        hist_predicted, bins  = np.histogram(features[:,feature_names.index(var)], np_binning, weights=w[0,0,:]*wpp[l,k,:])\n",
    "                        #plt.step(bins[1:],hist_predicted/hist_truth_0, label='truth', linestyle=('solid'), color=hist_colors[a[l,k]+len(coefficients)])\n",
    "                        plt.step(bins[1:],hist_predicted, label='truth', linestyle=('solid'), color=hist_colors[a[l,k]+len(coefficients)])\n",
    "                        plt.legend(['Yield ' + label, 'Truth '+ label, 'Predicted '+ label])\n",
    "                        plt.savefig(os.path.join(plot_directory, \"_epoch_%05i_%s_\"%(epoch, var)+label+\".png\"))\n",
    "                        plt.close()\n",
    "                    \n",
    "                \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee2e7e3c-4748-4eab-aa8c-7dfd26a70454",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(coefficients)+1):\n",
    "    pTFiles=[]\n",
    "    for file in os.listdir(os.getcwd()+'/'+ plot_directory):\n",
    "        # check only text files\n",
    "        string=\"__\" + coefficients[i-1] + \".png\"\n",
    "        if file.endswith(string):\n",
    "            pTFiles.append(file)\n",
    "            #print(file)\n",
    "    frames=[]\n",
    "    for image in pTFiles:\n",
    "        new_frame = Image.open(os.getcwd()+'/'+ plot_directory +'/'+image)\n",
    "        frames.append(new_frame)\n",
    "\n",
    "    frames[0].save(plot_directory + '__'+ coefficients[i-1]+'.gif', format='GIF',\n",
    "                   append_images=frames[1:],\n",
    "                   save_all=True,\n",
    "                   duration=200, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eac9cf0-79ad-4313-96ef-c4a7ae14f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(0,len(coefficients)):\n",
    "    for k in range(0,len(coefficients)):\n",
    "        label=combinations2[a[l,k]+len(coefficients)+1][0]+ '_' + combinations2[a[l,k]+len(coefficients)+1][1]\n",
    "        pTFiles=[]\n",
    "        for file in os.listdir(os.getcwd()+'/'+ plot_directory):\n",
    "            # check only text files\n",
    "            string=\"_\"+label+\".png\"\n",
    "            if file.endswith(string):\n",
    "                pTFiles.append(file)\n",
    "                #print(file)\n",
    "        frames=[]\n",
    "        for image in pTFiles:\n",
    "            new_frame = Image.open(os.getcwd()+'/'+ plot_directory +'/'+image)\n",
    "            frames.append(new_frame)\n",
    "\n",
    "        frames[0].save(plot_directory + '__'+ label+'.gif', format='GIF',\n",
    "                       append_images=frames[1:],\n",
    "                       save_all=True,\n",
    "                       duration=200, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d1c742-cde3-44d7-9fa8-b14ecae514a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda-ptur]",
   "language": "python",
   "name": "conda-env-conda-ptur-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
