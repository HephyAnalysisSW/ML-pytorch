{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9736663f-41bd-4b08-954b-fd212fffbc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import ROOT\n",
    "from Tools import syncer \n",
    "from Tools import user\n",
    "from Tools import helpers\n",
    "import itertools\n",
    "\n",
    "from PIL import Image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92336f08-671b-492c-8265-0dc5cbe7f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self,nfeatures,coefficient_names):\n",
    "        self.nfeatures         = nfeatures\n",
    "        self.coefficient_names = coefficient_names\n",
    "        self.combination_list=list(itertools.chain.from_iterable(itertools.combinations_with_replacement(self.coefficient_names, i) for i in np.arange(0,3)))\n",
    "        self.n_hat = {combination: self.make_NN() for combination in self.combination_list}\n",
    "        \n",
    "    def make_NN(self, hidden_layers  = [32, 32, 32, 32]):\n",
    "        '''\n",
    "        Creates the Neural Network Architecture\n",
    "        '''\n",
    "        model_nn = [torch.nn.BatchNorm1d(self.nfeatures), torch.nn.ReLU(), torch.nn.Linear(self.nfeatures, hidden_layers[0])]\n",
    "        for i_layer, layer in enumerate(hidden_layers):\n",
    "            model_nn.append(torch.nn.Linear(hidden_layers[i_layer], hidden_layers[i_layer+1] if i_layer+1<len(hidden_layers) else 1))\n",
    "            if i_layer+1<len(hidden_layers):\n",
    "                model_nn.append( torch.nn.ReLU() )\n",
    "        return torch.nn.Sequential(*model_nn)\n",
    "\n",
    "    def evaluate_NN(self, features):\n",
    "        '''Evaluate Neural Network: The zeroth dimension of features is the number of data points and and the first dimension\n",
    "        is the number of features(variables). Returns the output of the NNs of dimensions: (noutput,ndatapoints)\n",
    "        '''\n",
    "        noutputs=len(self.combination_list)\n",
    "        ndatapoints=features.shape[0]\n",
    "        \n",
    "        output=torch.zeros((noutputs,ndatapoints))\n",
    "        for i in range(noutputs):\n",
    "            x=self.n_hat[self.combination_list[i]](features)\n",
    "            if i==0:\n",
    "                output[i,:]=1\n",
    "            else:\n",
    "                output[i,:]=torch.flatten(x)            \n",
    "        return output\n",
    "\n",
    "    def predict_r_hat2(self, predictions,eft):\n",
    "        return torch.add( \n",
    "        torch.sum( torch.stack( [(1. + predictions[(c,)]*eft[c])**2 for c in coefficients ]), dim=0),\n",
    "        torch.sum( torch.stack( [torch.sum( torch.stack( [ predictions[(c_1,c_2)]*eft[c_2] for c_2 in coefficients[i_c_1:] ]), dim=0)**2 for i_c_1, c_1 in enumerate(coefficients) ] ), dim=0 ) )   \n",
    "\n",
    "    def save(self,fileName):\n",
    "        outfile = open(fileName,'wb')\n",
    "        pickle.dump(self, outfile)\n",
    "        outfile.close()\n",
    "        \n",
    "    @classmethod\n",
    "    def load(self, fileName):\n",
    "        infile = open(fileName,'rb')\n",
    "        print(fileName)\n",
    "        new_dict = pickle.load(infile)\n",
    "        infile.close()\n",
    "        return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f300bba0-ec54-4e0e-9b9e-4e12718c1653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weight_ratio(weights, **kwargs):\n",
    "    eft      = kwargs\n",
    "    result = torch.ones(len(weights[()])) \n",
    "    for combination in combinations:\n",
    "        if len(combination)==1:\n",
    "            result += eft[combination[0]]*weights[combination]/weights[()]\n",
    "        elif len(combination)==2:# add up without the factor 1/2 because off diagonals are only summed in upper triangle \n",
    "            result += (0.5 if len(set(combination))==1 else 1.)*eft[combination[0]]*eft[combination[1]]*weights[combination]/weights[()]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecb14fda-63c7-40fd-8558-e73b040ae190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functional\n",
    "def f_loss(r_NN, features, predictions, base_points):\n",
    "    loss = -0.5*weights[()].sum()\n",
    "    for i_base_point, base_point in enumerate(base_points):\n",
    "        fhat  = 1./(1. + r_NN.predict_r_hat2(predictions, base_point))\n",
    "        loss += ( torch.tensor(weights[()])*( -0.25 + base_point_weight_ratios[i_base_point]*fhat**2 + (1-fhat)**2 ) ).sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3ba614a-e60f-487b-99c0-3312429a1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_directory=\"13_7_2022\"\n",
    "nEvents=30000\n",
    "learning_rate = 1e-3\n",
    "device        = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_epoch       = 3000\n",
    "plot_every    = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a4a711d-6147-429a-8b5e-5d37a53e2526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested 30000 events. Simulated 30000 events and 30000 survive pT_min cut of 0.\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "import ZH_Nakamura\n",
    "\n",
    "ZH_Nakamura.feature_names = ZH_Nakamura.feature_names[0:6] # restrict features\n",
    "features   = ZH_Nakamura.getEvents(nEvents)[:,0:6]\n",
    "feature_names  = ZH_Nakamura.feature_names\n",
    "plot_options   = ZH_Nakamura.plot_options\n",
    "plot_vars      = ZH_Nakamura.feature_names\n",
    "\n",
    "mask       = (features[:,feature_names.index('pT')]<900) & (features[:,feature_names.index('sqrt_s_hat')]<1800) \n",
    "features = features[mask]\n",
    "\n",
    "nfeatures = len(features[0]) \n",
    "weights    = ZH_Nakamura.getWeights(features, ZH_Nakamura.make_eft())\n",
    "\n",
    "\n",
    "#pT=features[:,feature_names.index('pT')]\n",
    "\n",
    "\n",
    "\n",
    "coefficients   =  ( 'cHW', 'cHWtil') \n",
    "combinations   =  [(), ('cHW',), ('cHWtil',), ('cHW','cHW'), ('cHW','cHWtil'),('cHWtil','cHWtil')]\n",
    "combinations2 = [(), ('cHW',), ('cHWtil',), ('cHW','cHW'), ('cHW','cHWtil'),('cHWtil','cHW'),('cHWtil','cHWtil')]\n",
    "base_points = [ {'cHW':value1, 'cHWtil':value2} for value1 in [-1.5, -.8, .2, 0., .2, .8, 1.5]  for value2 in [-1.5, -.8, .2, 0, .2, .8, 1.5]]\n",
    "\n",
    "rows, columns = np.triu_indices(len(coefficients)+1)\n",
    "w=np.zeros((len(coefficients)+1,len(coefficients)+1,features.shape[0]))\n",
    "\n",
    "for i in range(0,len(combinations)):\n",
    "    w[rows[i],columns[i],:]=torch.from_numpy(weights[combinations[i]]).float().to(device)\n",
    "    if rows[i]!=columns[i]:\n",
    "        w[columns[i],rows[i],:]=w[rows[i],columns[i],:]\n",
    "\n",
    "features = torch.from_numpy(features).float().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a51735a-584a-4025-8521-fcef189132bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = tuple(filter( lambda coeff: coeff in coefficients, list(coefficients))) \n",
    "combinations = tuple(filter( lambda comb: all([c in coefficients for c in comb]), combinations)) \n",
    "\n",
    "base_points    = list(map( lambda b:ZH_Nakamura.make_eft(**b), base_points ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "350f60f1-d0c7-40b8-a9f9-c7edcfe06ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_NN=NN(nfeatures,coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58893910-4759-43c7-ab89-0582a047c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_point_weight_ratios = list( map( lambda base_point: make_weight_ratio( weights, **base_point ), base_points ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd9a6e4-c7cc-4c1d-b0ef-794086eedb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(sum([list(model.parameters()) for model in r_NN.n_hat.values()],[]), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "tex = ROOT.TLatex()\n",
    "tex.SetNDC()\n",
    "tex.SetTextSize(0.04)\n",
    "hist_colors=['b','g', 'r', 'c', 'm','y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cdddd70-6982-4579-a182-0208a4f7224e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99 loss 2.577893692619988\n",
      "epoch 199 loss 2.566674588236542\n",
      "epoch 299 loss 2.5505626438403124\n",
      "epoch 399 loss 2.528638642200234\n",
      "epoch 499 loss 2.5106022076374885\n",
      "epoch 599 loss 2.5051012142753883\n",
      "epoch 699 loss 2.503134481915266\n",
      "epoch 799 loss 2.5021203458470658\n",
      "epoch 899 loss 2.4991661541704198\n",
      "epoch 999 loss 2.4958345277577307\n",
      "epoch 1099 loss 2.495527082833928\n",
      "epoch 1199 loss 2.495366250644571\n",
      "epoch 1299 loss 2.4952361218816814\n",
      "epoch 1399 loss 2.495107610877079\n",
      "epoch 1499 loss 2.494428427622089\n",
      "epoch 1599 loss 2.493840135930059\n",
      "epoch 1699 loss 2.493532989961228\n",
      "epoch 1799 loss 2.4933820018023987\n",
      "epoch 1899 loss 2.4931926864943197\n",
      "epoch 1999 loss 2.4928787140766877\n",
      "epoch 2099 loss 2.4925531581717246\n",
      "epoch 2199 loss 2.492238707806419\n",
      "epoch 2299 loss 2.49202000252544\n",
      "epoch 2399 loss 2.4917508559911994\n",
      "epoch 2499 loss 2.491469412558504\n",
      "epoch 2599 loss 2.4911365995000625\n",
      "epoch 2699 loss 2.4909434316325076\n",
      "epoch 2799 loss 2.4903281493845286\n",
      "epoch 2899 loss 2.4901061011425667\n",
      "epoch 2999 loss 2.4896578414599935\n"
     ]
    }
   ],
   "source": [
    "for nn in r_NN.n_hat.values():\n",
    "    nn.train()\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    #print(\"epoch: \", epoch)\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    predictions = {combination:r_NN.n_hat[combination](features).squeeze() for combination in combinations}\n",
    "    \n",
    "    #rows, columns = np.triu_indices(len(coefficients)+1)\n",
    "    w_predicted=np.zeros((len(coefficients)+1, len(coefficients)+1, features.shape[0]))      \n",
    "    for i in range(1,len(combinations)):\n",
    "        w_predicted[rows[i],columns[i],:]=predictions[combinations[i]].squeeze().cpu().detach().numpy()\n",
    "        if rows[i]!=columns[i]:\n",
    "            w_predicted[columns[i],rows[i],:]=w_predicted[rows[i],columns[i],:]\n",
    "    \n",
    "    wp=np.zeros((len(coefficients),features.shape[0]))\n",
    "    for i in range(0,len(coefficients)):\n",
    "        wp[i,:]=2*np.sum(w_predicted[:,i+1,:],0)\n",
    "    \n",
    "    wpp=np.zeros((len(coefficients),len(coefficients),len(coefficients),features.shape[0]))\n",
    "    for i in range(0,len(coefficients)):\n",
    "        for l in range(0,len(coefficients)):\n",
    "            for k in range(0,len(coefficients)):\n",
    "                wpp[i,l,k,:]=2*w_predicted[i,l+1,:]*w_predicted[i,k+1,:]\n",
    "    \n",
    "    wpp=np.sum(wpp,0)\n",
    "               \n",
    "    # Compute and print loss.\n",
    "    loss = f_loss(r_NN,features, predictions,base_points)\n",
    "    losses.append(loss.item())\n",
    "    if epoch % 100 == 99:\n",
    "        print(\"epoch\", epoch, \"loss\",  loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "\n",
    "    if (epoch % plot_every)==0:\n",
    "        with torch.no_grad():\n",
    "                \n",
    "            #print(\"epoch\", epoch, \"loss inside loop\",  loss_train.item())\n",
    "            #print(\"epoch\", epoch, \"test loss\",  loss_test.item())\n",
    "            \n",
    "            for var in ['pT']:\n",
    "                binning     = plot_options[var]['binning']\n",
    "                np_binning= np.linspace(binning[1], binning[2], 1+binning[0])\n",
    "                \n",
    "                # Linear Terms\n",
    "                hist_truth_0, bins  = np.histogram(features[:,feature_names.index(var)], np_binning, weights=w[0,0,:])\n",
    "                bins = bins[1:]\n",
    "\n",
    "               \n",
    "                \n",
    "                \n",
    "                for i in range(1,len(coefficients)+1):\n",
    "                    #plt.hist(features[:,feature_names.index(var)], bins, histtype='step', label='Yield',weights=w[0,0,:],color='black')\n",
    "                    plt.hist(features[:,feature_names.index(var)], bins, histtype='step', label='truth',weights=w[0,i,:],linestyle=('dashed'), color=hist_colors[i])\n",
    "                    plt.hist(features[:,feature_names.index(var)], bins, histtype='step', label='predicted',weights=w[0,0,:]*wp[i-1,:], color = hist_colors[i])\n",
    "                    plt.legend(['Yield ' + coefficients[i-1], 'Truth '+ coefficients[i-1], 'Predicted '+ coefficients[i-1]])\n",
    "                    plt.legend(['Truth '+ coefficients[i-1], 'Predicted '+ coefficients[i-1]])\n",
    "                    plt.savefig(os.path.join(plot_directory, \"_epoch_%05i_%s__\"%(epoch, var)+coefficients[i-1] + \".png\" ))\n",
    "                    plt.close()\n",
    "                \n",
    "                \n",
    "                a = np.arange(len(coefficients)**2).reshape(len(coefficients),len(coefficients))\n",
    "                for l in range(0,len(coefficients)):\n",
    "                    for k in range(0,len(coefficients)):\n",
    "                        label=combinations2[a[l,k]+len(coefficients)+1][0]+ '_' + combinations2[a[l,k]+len(coefficients)+1][1]\n",
    "                        #plt.hist(features[:,feature_names.index(var)], bins, histtype='step', label='Yield',weights=w[0,0,:],color='black')\n",
    "                        plt.hist(features[:,feature_names.index(var)], bins, histtype='step', label='truth',weights=w[l+1,k+1,:],linestyle=('dashed'), color=hist_colors[a[l,k]+len(coefficients)])\n",
    "                        plt.hist(features[:,feature_names.index(var)], bins, histtype='step', label='predicted',weights=w[0,0,:]*wpp[l,k,:], color = hist_colors[a[l,k]+len(coefficients)])\n",
    "                        #plt.legend(['Yield ' + label, 'Truth '+ label, 'Predicted '+ label])\n",
    "                        plt.legend(['Truth '+ label, 'Predicted '+ label])\n",
    "                        plt.savefig(os.path.join(plot_directory, \"_epoch_%05i_%s_\"%(epoch, var)+label+\".png\"))\n",
    "                        plt.close()\n",
    "                    \n",
    "                \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee2e7e3c-4748-4eab-aa8c-7dfd26a70454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_epoch_00000_pT__cHW.png\n",
      "_epoch_00100_pT__cHW.png\n",
      "_epoch_00200_pT__cHW.png\n",
      "_epoch_00300_pT__cHW.png\n",
      "_epoch_00400_pT__cHW.png\n",
      "_epoch_00500_pT__cHW.png\n",
      "_epoch_00600_pT__cHW.png\n",
      "_epoch_00700_pT__cHW.png\n",
      "_epoch_00800_pT__cHW.png\n",
      "_epoch_00900_pT__cHW.png\n",
      "_epoch_01000_pT__cHW.png\n",
      "_epoch_01100_pT__cHW.png\n",
      "_epoch_01200_pT__cHW.png\n",
      "_epoch_01300_pT__cHW.png\n",
      "_epoch_01400_pT__cHW.png\n",
      "_epoch_01500_pT__cHW.png\n",
      "_epoch_01600_pT__cHW.png\n",
      "_epoch_01700_pT__cHW.png\n",
      "_epoch_01800_pT__cHW.png\n",
      "_epoch_01900_pT__cHW.png\n",
      "_epoch_02000_pT__cHW.png\n",
      "_epoch_02100_pT__cHW.png\n",
      "_epoch_02200_pT__cHW.png\n",
      "_epoch_02300_pT__cHW.png\n",
      "_epoch_02400_pT__cHW.png\n",
      "_epoch_02500_pT__cHW.png\n",
      "_epoch_02600_pT__cHW.png\n",
      "_epoch_02700_pT__cHW.png\n",
      "_epoch_02800_pT__cHW.png\n",
      "_epoch_02900_pT__cHW.png\n",
      "_epoch_00000_pT__cHWtil.png\n",
      "_epoch_00100_pT__cHWtil.png\n",
      "_epoch_00200_pT__cHWtil.png\n",
      "_epoch_00300_pT__cHWtil.png\n",
      "_epoch_00400_pT__cHWtil.png\n",
      "_epoch_00500_pT__cHWtil.png\n",
      "_epoch_00600_pT__cHWtil.png\n",
      "_epoch_00700_pT__cHWtil.png\n",
      "_epoch_00800_pT__cHWtil.png\n",
      "_epoch_00900_pT__cHWtil.png\n",
      "_epoch_01000_pT__cHWtil.png\n",
      "_epoch_01100_pT__cHWtil.png\n",
      "_epoch_01200_pT__cHWtil.png\n",
      "_epoch_01300_pT__cHWtil.png\n",
      "_epoch_01400_pT__cHWtil.png\n",
      "_epoch_01500_pT__cHWtil.png\n",
      "_epoch_01600_pT__cHWtil.png\n",
      "_epoch_01700_pT__cHWtil.png\n",
      "_epoch_01800_pT__cHWtil.png\n",
      "_epoch_01900_pT__cHWtil.png\n",
      "_epoch_02000_pT__cHWtil.png\n",
      "_epoch_02100_pT__cHWtil.png\n",
      "_epoch_02200_pT__cHWtil.png\n",
      "_epoch_02300_pT__cHWtil.png\n",
      "_epoch_02400_pT__cHWtil.png\n",
      "_epoch_02500_pT__cHWtil.png\n",
      "_epoch_02600_pT__cHWtil.png\n",
      "_epoch_02700_pT__cHWtil.png\n",
      "_epoch_02800_pT__cHWtil.png\n",
      "_epoch_02900_pT__cHWtil.png\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,len(coefficients)+1):\n",
    "    pTFiles=[]\n",
    "    for file in os.listdir(os.getcwd()+'/13_7_2022'):\n",
    "        # check only text files\n",
    "        string=\"__\" + coefficients[i-1] + \".png\"\n",
    "        if file.endswith(string):\n",
    "            pTFiles.append(file)\n",
    "            #print(file)\n",
    "    frames=[]\n",
    "    for image in pTFiles:\n",
    "        new_frame = Image.open(os.getcwd()+'/13_7_2022/'+image)\n",
    "        frames.append(new_frame)\n",
    "\n",
    "    frames[0].save('13_7_2022__'+ coefficients[i-1]+'.gif', format='GIF',\n",
    "                   append_images=frames[1:],\n",
    "                   save_all=True,\n",
    "                   duration=200, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3eac9cf0-79ad-4313-96ef-c4a7ae14f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(0,len(coefficients)):\n",
    "    for k in range(0,len(coefficients)):\n",
    "        label=combinations2[a[l,k]+len(coefficients)+1][0]+ '_' + combinations2[a[l,k]+len(coefficients)+1][1]\n",
    "        pTFiles=[]\n",
    "        for file in os.listdir(os.getcwd()+'/13_7_2022'):\n",
    "            # check only text files\n",
    "            string=\"_\"+label+\".png\"\n",
    "            if file.endswith(string):\n",
    "                pTFiles.append(file)\n",
    "                #print(file)\n",
    "        frames=[]\n",
    "        for image in pTFiles:\n",
    "            new_frame = Image.open(os.getcwd()+'/13_7_2022/'+image)\n",
    "            frames.append(new_frame)\n",
    "\n",
    "        frames[0].save('13_7_2022__'+ label+'.gif', format='GIF',\n",
    "                       append_images=frames[1:],\n",
    "                       save_all=True,\n",
    "                       duration=200, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d7bb3d-397e-42d6-a104-340ff32fca76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda-ptur]",
   "language": "python",
   "name": "conda-env-conda-ptur-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
