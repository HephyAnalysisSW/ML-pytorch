{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46154223-1a7b-4eb5-a15a-b7cf175ff637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.24/06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import ROOT\n",
    "from Tools import syncer \n",
    "from Tools import user\n",
    "from Tools import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "702858cd-5234-43a3-9fb8-08f2946fa95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_directory=\"v1\"\n",
    "coefficients=['cHW', 'cHWtil', 'cHQ3']\n",
    "nEvents=30000\n",
    "learning_rate = 1e-3\n",
    "device        = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_epoch       = 10000\n",
    "plot_every    = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf2ba1d7-6e52-47a4-98d8-94ad5a98a8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested 30000 events. Simulated 30000 events and 30000 survive pT_min cut of 0.\n",
      "dict_keys([(), ('cHQ3',), ('cHW',), ('cHWtil',), ('cHQ3', 'cHQ3'), ('cHW', 'cHW'), ('cHWtil', 'cHWtil'), ('cHQ3', 'cHW'), ('cHQ3', 'cHWtil'), ('cHW', 'cHWtil')])\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "import ZH_Nakamura \n",
    "ZH_Nakamura.feature_names = ZH_Nakamura.feature_names[0:6] # restrict features\n",
    "features   = ZH_Nakamura.getEvents(nEvents)[:,0:6]\n",
    "feature_names  = ZH_Nakamura.feature_names\n",
    "plot_options   = ZH_Nakamura.plot_options\n",
    "plot_vars      = ZH_Nakamura.feature_names\n",
    "\n",
    "mask       = (features[:,feature_names.index('pT')]<900) & (features[:,feature_names.index('sqrt_s_hat')]<1800) \n",
    "features = features[mask]\n",
    "\n",
    "n_features = len(features[0]) \n",
    "weights    = ZH_Nakamura.getWeights(features, ZH_Nakamura.make_eft() )\n",
    "\n",
    "print(weights.keys())\n",
    "WC = 'cHW'\n",
    "features_train = torch.from_numpy(features).float().to(device)\n",
    "\n",
    "#coefficients   = ('cHW', ) \n",
    "coefficients   =  ( 'cHW', 'cHWtil', 'cHQ3') \n",
    "combinations   =  [ ('cHW',), ('cHWtil',), ('cHQ3',), ('cHW', 'cHW'), ('cHWtil', 'cHWtil'), ('cHQ3', 'cHQ3'), ('cHW', 'cHWtil'), ('cHQ3', 'cHW'), ('cHQ3', 'cHWtil')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14861b0d-41cc-4e92-94ec-f246746b63bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cHW'],\n",
       " ['cHWtil'],\n",
       " ['cHQ3'],\n",
       " ['cHW', 'cHW'],\n",
       " ['cHWtil', 'cHWtil'],\n",
       " ['cHQ3', 'cHQ3'],\n",
       " ['cHW', 'cHWtil'],\n",
       " ['cHQ3', 'cHW'],\n",
       " ['cHQ3', 'cHWtil']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(sorted, combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e677ce6d-ee1b-4bb9-8fc6-d1b1bfc3116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_points = [ {'cHW':value1, 'cHWtil':value2} for value1 in [-1.5, -.8, .2, 0., .2, .8, 1.5]  for value2 in [-1.5, -.8, .2, 0, .2, .8, 1.5]]\n",
    "base_points = list(filter( (lambda point: all([ coeff in coefficients or (not (coeff in point.keys() and point[coeff]!=0)) for coeff in point.keys()]) and any(map(bool, point.values()))), base_points)) \n",
    "\n",
    "coefficients = tuple(filter( lambda coeff: coeff in coefficients, list(coefficients))) \n",
    "combinations = tuple(filter( lambda comb: all([c in coefficients for c in comb]), combinations)) \n",
    "\n",
    "#base_points    = [ { 'cHW':-1.5 }, {'cHW':-.8}, {'cHW':-.4}, {'cHW':-.2}, {'cHW':.2}, {'cHW':.4}, {'cHW':.8}, {'cHW':1.5} ]\n",
    "#base_points   += [ { 'cHWtil':-1.5 }, {'cHWtil':-.8}, {'cHWtil':-.4}, {'cHWtil':-.2}, {'cHWtil':.2}, {'cHWtil':.4}, {'cHWtil':.8}, {'cHWtil':1.5} ]\n",
    "#base_points   += [ { 'cHQ3':-.15 }, {'cHQ3':-.08}, {'cHQ3':-.04}, {'cHQ3':-.02}, {'cHQ3':.02}, {'cHQ3':.04}, {'cHQ3':.08}, {'cHQ3':0.15} ]\n",
    "\n",
    "\n",
    "base_points    = list(map( lambda b:ZH_Nakamura.make_eft(**b), base_points ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ee350f-ae53-49d9-bc35-df77390c090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make standard NN \n",
    "def make_NN( hidden_layers  = [32, 32, 32, 32] ):\n",
    "    model_nn = [torch.nn.BatchNorm1d(n_features), torch.nn.ReLU(), torch.nn.Linear(n_features, hidden_layers[0])]\n",
    "    for i_layer, layer in enumerate(hidden_layers):\n",
    "\n",
    "        model_nn.append(torch.nn.Linear(hidden_layers[i_layer], hidden_layers[i_layer+1] if i_layer+1<len(hidden_layers) else 1))\n",
    "        if i_layer+1<len(hidden_layers):\n",
    "            model_nn.append( torch.nn.ReLU() )\n",
    "\n",
    "    return torch.nn.Sequential(*model_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af09886b-6965-4905-b0c6-1271f69717e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hat = { combination:make_NN() for combination in combinations }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b45a029-8d55-4e2e-84ab-cc3cefdc73db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cHW', 'cHWtil', 'cHQ3')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43a4fde6-636c-46a9-b904-d530e3c30646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_hat( predictions, eft ):\n",
    "    \n",
    "    term1=torch.sum( torch.stack( [(1. + predictions[(c,)]*eft[c])**2 for c in coefficients ]), dim=0)\n",
    "    \n",
    "    term2=torch.sum( torch.stack( [torch.sum( torch.stack( [ predictions[tuple(sorted((c_1,c_2)))]*eft[c_2] for c_1 in coefficients[i_c_2:] ]), dim=0)**2 for i_c_2, c_2 in enumerate(coefficients) ] ), dim=0 )\n",
    "    \n",
    "    return torch.add(term1,term2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30cf4a21-a841-4b5f-b08d-1b95f574b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_hat2( predictions, eft ):\n",
    "    \n",
    "    term1=torch.sum( torch.stack( [(1. + predictions[(c,)]*eft[c])**2 for c in coefficients ]), dim=0)\n",
    "    term2=0\n",
    "    \n",
    "    for i_c_1, c_2 in enumerate(coefficients):\n",
    "        print(i_c_1)\n",
    "        print(c_2)\n",
    "        for c_1 in coefficients[i_c_1:]:\n",
    "            print(c_1)\n",
    "            term2+=predictions[(c_1,c_2)]*eft[c_2]\n",
    "    \n",
    "    return torch.add(term1,term2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39203306-d957-4066-8bee-1afeb83d500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weight_ratio( weights, **kwargs ):\n",
    "    eft      = kwargs\n",
    "    result = torch.ones(len(weights[()])) \n",
    "    for combination in combinations:\n",
    "        if len(combination)==1:\n",
    "            result += eft[combination[0]]*weights[combination]/weights[()]\n",
    "        elif len(combination)==2:# add up without the factor 1/2 because off diagonals are only summed in upper triangle \n",
    "            result += (0.5 if len(set(combination))==1 else 1.)*eft[combination[0]]*eft[combination[1]]*weights[combination]/weights[()]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bb173fa-4037-4e2d-adae-7ef6199f264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_point_weight_ratios = list( map( lambda base_point: make_weight_ratio( weights, **base_point ), base_points ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e01e82b7-7131-48ce-9e06-9ac67413a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functional\n",
    "def f_loss(predictions):\n",
    "    loss = -0.5*weights[()].sum()\n",
    "    for i_base_point, base_point in enumerate(base_points):\n",
    "        #fhat  = 1./(1. + ( 1. + theta*t_output)**2 + (theta*s_output)**2 )\n",
    "        fhat  = 1./(1. + r_hat(predictions, base_point) )\n",
    "        loss += ( torch.tensor(weights[()])*( -0.25 + base_point_weight_ratios[i_base_point]*fhat**2 + (1-fhat)**2 ) ).sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4ce835c-bdfa-4f34-96ca-0a0488156eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(sum([list(model.parameters()) for model in n_hat.values()],[]), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "tex = ROOT.TLatex()\n",
    "tex.SetNDC()\n",
    "tex.SetTextSize(0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be1a81-8071-41e7-b38e-4b34a7ddcaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Loss:\n",
      "2.916257642168629\n",
      "epoch 99 loss 2.9140950808550135\n",
      "Printing Loss:\n",
      "2.9140903923448715\n",
      "epoch 199 loss 2.913524966992073\n",
      "Printing Loss:\n",
      "2.9135181979121283\n",
      "epoch 299 loss 2.912742944386816\n",
      "Printing Loss:\n",
      "2.9127299773171695\n",
      "epoch 399 loss 2.9120466900471684\n",
      "Printing Loss:\n",
      "2.912040527503755\n",
      "epoch 499 loss 2.911566868633191\n",
      "Printing Loss:\n",
      "2.911562600300274\n",
      "epoch 599 loss 2.9112066757578012\n",
      "Printing Loss:\n",
      "2.91121034278949\n",
      "epoch 699 loss 2.9109462586411343\n",
      "Printing Loss:\n",
      "2.910943530782047\n",
      "epoch 799 loss 2.910794355458405\n",
      "Printing Loss:\n",
      "2.9107933374659387\n",
      "epoch 899 loss 2.910690134379129\n",
      "Printing Loss:\n",
      "2.910690934322541\n",
      "epoch 999 loss 2.910621727011666\n",
      "Printing Loss:\n",
      "2.9106261499087362\n",
      "epoch 1099 loss 2.910516540662193\n",
      "Printing Loss:\n",
      "2.9105150512777525\n",
      "epoch 1199 loss 2.9104762604672283\n",
      "Printing Loss:\n",
      "2.9104563759603517\n",
      "epoch 1299 loss 2.9105099943107025\n",
      "Printing Loss:\n",
      "2.9105162745623363\n",
      "epoch 1399 loss 2.91036342294758\n",
      "Printing Loss:\n",
      "2.9103689415450558\n",
      "epoch 1499 loss 2.910278991762051\n",
      "Printing Loss:\n",
      "2.910278541254325\n",
      "epoch 1599 loss 2.9102303276869486\n",
      "Printing Loss:\n",
      "2.910227984940239\n",
      "epoch 1699 loss 2.9101876265032462\n",
      "Printing Loss:\n",
      "2.910188828438583\n",
      "epoch 1799 loss 2.9101439811706915\n",
      "Printing Loss:\n",
      "2.9101377381372036\n",
      "epoch 1899 loss 2.9100792541704377\n",
      "Printing Loss:\n",
      "2.9100813812741886\n",
      "epoch 1999 loss 2.9100389199566297\n",
      "Printing Loss:\n",
      "2.9100428855805727\n",
      "epoch 2099 loss 2.9099427264615216\n",
      "Printing Loss:\n",
      "2.90995186514595\n",
      "epoch 2199 loss 2.909870256441843\n",
      "Printing Loss:\n",
      "2.9098769696928883\n",
      "epoch 2299 loss 2.9098225304087517\n",
      "Printing Loss:\n",
      "2.909842290337834\n",
      "epoch 2399 loss 2.9096791768108097\n",
      "Printing Loss:\n",
      "2.909671272226653\n",
      "epoch 2499 loss 2.9095490774992783\n",
      "Printing Loss:\n",
      "2.909543458177443\n",
      "epoch 2599 loss 2.9095386901113423\n",
      "Printing Loss:\n",
      "2.9095385568689225\n",
      "epoch 2699 loss 2.90946573975239\n",
      "Printing Loss:\n",
      "2.9094688990447466\n",
      "epoch 2799 loss 2.909414290159463\n",
      "Printing Loss:\n",
      "2.909420609232515\n",
      "epoch 2899 loss 2.9093800077837697\n",
      "Printing Loss:\n",
      "2.9093839435341002\n",
      "epoch 2999 loss 2.90937919704635\n",
      "Printing Loss:\n",
      "2.9093616334369314\n",
      "epoch 3099 loss 2.909334688027057\n",
      "Printing Loss:\n",
      "2.9093338162001143\n",
      "epoch 3199 loss 2.90928953030459\n",
      "Printing Loss:\n",
      "2.909297134727351\n",
      "epoch 3299 loss 2.9092738181855333\n",
      "Printing Loss:\n",
      "2.9092786003546616\n",
      "epoch 3399 loss 2.9092326314766566\n",
      "Printing Loss:\n",
      "2.9092258018409134\n",
      "epoch 3499 loss 2.908895869569936\n",
      "Printing Loss:\n",
      "2.9089048529638655\n",
      "epoch 3599 loss 2.9087057532633924\n",
      "Printing Loss:\n",
      "2.9087159939691847\n",
      "epoch 3699 loss 2.908675253501745\n",
      "Printing Loss:\n",
      "2.908682174286047\n",
      "epoch 3799 loss 2.9086483706276107\n",
      "Printing Loss:\n",
      "2.908651314345183\n",
      "epoch 3899 loss 2.908569404467202\n",
      "Printing Loss:\n",
      "2.9085707192831523\n",
      "epoch 3999 loss 2.908556581318758\n",
      "Printing Loss:\n",
      "2.9085530094324104\n",
      "epoch 4099 loss 2.9085675381712996\n",
      "Printing Loss:\n",
      "2.908542245686322\n"
     ]
    }
   ],
   "source": [
    "# variables for ploting results\n",
    "for nn in n_hat.values():\n",
    "    nn.train()\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    predictions = {combination:n_hat[combination](features_train).squeeze() for combination in combinations}\n",
    "    #print (\"t\", pred_t.mean(), \"s\", pred_s.mean())\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = f_loss(predictions)\n",
    "    losses.append(loss.item())\n",
    "    if epoch % 100 == 99:\n",
    "        print(\"epoch\", epoch, \"loss\",  loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch % plot_every)==0:\n",
    "        with torch.no_grad():\n",
    "            print ('Printing Loss:')\n",
    "            print (loss.item())\n",
    "            for comb in combinations:\n",
    "                if len(comb)==1: continue\n",
    "\n",
    "                t1, t2 = comb\n",
    "\n",
    "                pred_t1 = n_hat[(t1,)](features_train).squeeze().cpu().detach().numpy()\n",
    "                if t1 == t2:\n",
    "                    pred_t2 = pred_t1\n",
    "                else: \n",
    "                    pred_t2 = n_hat[(t2,)](features_train).squeeze().cpu().detach().numpy()\n",
    "\n",
    "                pred_s  = n_hat[comb](features_train).squeeze().cpu().detach().numpy()\n",
    "\n",
    "                for var in plot_vars:\n",
    "                    binning   = plot_options[var]['binning']\n",
    "                    np_binning= np.linspace(binning[1], binning[2], 1+binning[0])\n",
    "\n",
    "\n",
    "                    w0_train  = weights[()]\n",
    "                    truth_0   = np.histogram(features_train[:,feature_names.index(var)], np_binning, weights=w0_train )\n",
    "\n",
    "                    wp1_train = weights[(t1,)]\n",
    "                    wp2_train = weights[(t2,)]\n",
    "                    truth_p1  = np.histogram(features_train[:,feature_names.index(var)], np_binning, weights=wp1_train )\n",
    "\n",
    "                    wpp_train = weights[comb]\n",
    "                    truth_pp  = np.histogram(features_train[:,feature_names.index(var)], np_binning, weights=wpp_train )\n",
    "\n",
    "\n",
    "                    #pred_0  = np.histogram(features_train[:,feature_names.index(var)], np_binning, weights=w0_train )\n",
    "                    pred_p1  = np.histogram(features_train[:,feature_names.index(var)], np_binning, weights=w0_train*2*pred_t1 )\n",
    "                    pred_pp = np.histogram(features_train[:,feature_names.index(var)], np_binning, weights=w0_train*2*(pred_t1*pred_t2+pred_s**2) )\n",
    "\n",
    "                    h_yield       = helpers.make_TH1F(truth_0)\n",
    "                    h_truth_p1    = helpers.make_TH1F(truth_p1)\n",
    "                    h_truth_p1    .Divide(h_yield) \n",
    "                    h_truth_pp    = helpers.make_TH1F(truth_pp)\n",
    "                    h_truth_pp    .Divide(h_yield) \n",
    "\n",
    "                    h_pred_p1      = helpers.make_TH1F(pred_p1)\n",
    "                    h_pred_p1      .Divide(h_yield) \n",
    "                    h_pred_pp     = helpers.make_TH1F(pred_pp)\n",
    "                    h_pred_pp     .Divide(h_yield) \n",
    "\n",
    "                    l = ROOT.TLegend(0.3,0.7,0.9,0.95)\n",
    "                    l.SetNColumns(2)\n",
    "                    l.SetFillStyle(0)\n",
    "                    l.SetShadowColor(ROOT.kWhite)\n",
    "                    l.SetBorderSize(0)\n",
    "\n",
    "                    h_yield      .SetLineColor(ROOT.kGray+2) \n",
    "                    h_truth_p1   .SetLineColor(ROOT.kBlue) \n",
    "                    h_truth_pp   .SetLineColor(ROOT.kRed) \n",
    "                    h_pred_p1    .SetLineColor(ROOT.kBlue) \n",
    "                    h_pred_pp    .SetLineColor(ROOT.kRed) \n",
    "                    h_yield      .SetMarkerColor(ROOT.kGray+2) \n",
    "                    h_truth_p1   .SetMarkerColor(ROOT.kBlue) \n",
    "                    h_truth_pp   .SetMarkerColor(ROOT.kRed) \n",
    "                    h_pred_p1    .SetMarkerColor(ROOT.kBlue) \n",
    "                    h_pred_pp    .SetMarkerColor(ROOT.kRed) \n",
    "                    h_yield      .SetMarkerStyle(0)\n",
    "                    h_truth_p1   .SetMarkerStyle(0)\n",
    "                    h_truth_pp   .SetMarkerStyle(0)\n",
    "                    h_pred_p1    .SetMarkerStyle(0)\n",
    "                    h_pred_pp    .SetMarkerStyle(0)\n",
    "                    h_truth_p1   .SetLineStyle(ROOT.kDashed) \n",
    "                    h_truth_pp   .SetLineStyle(ROOT.kDashed)\n",
    "\n",
    "                    max_ = max( map( lambda h:h.GetMaximum(), [ h_truth_p1, h_truth_pp ] ))\n",
    "\n",
    "                    l.AddEntry(h_truth_p1  , \"D(%s) (truth)\"%( t1 ) ) \n",
    "                    l.AddEntry(h_pred_p1   , \"D(%s) (pred)\"%( t1 ) ) \n",
    "                    if t1!=t2:\n",
    "                        truth_p2     = np.histogram(features_train[:,feature_names.index(var)], np_binning, weights=wp2_train )\n",
    "                        pred_p2      = np.histogram(features_train[:,feature_names.index(var)], np_binning, weights=w0_train*2*pred_t2 )\n",
    "                        h_truth_p2   = helpers.make_TH1F(truth_p2)\n",
    "                        h_truth_p2   .Divide(h_yield) \n",
    "                        h_pred_p2    = helpers.make_TH1F(pred_p2)\n",
    "                        h_pred_p2    .Divide(h_yield)\n",
    "                        h_truth_p2   .SetLineColor(ROOT.kGreen) \n",
    "                        h_pred_p2    .SetLineColor(ROOT.kGreen) \n",
    "                        h_truth_p2   .SetMarkerColor(ROOT.kGreen) \n",
    "                        h_pred_p2    .SetMarkerColor(ROOT.kGreen) \n",
    "                        h_truth_p2   .SetMarkerStyle(0)\n",
    "                        h_pred_p2    .SetMarkerStyle(0)\n",
    "                        h_truth_p2   .SetLineStyle(ROOT.kDashed)\n",
    "\n",
    "                        max_ = max( map( lambda h:h.GetMaximum(), [ h_truth_p1, h_truth_p2, h_truth_pp ] ))\n",
    "\n",
    "                        l.AddEntry(h_truth_p2   , \"D(%s) (truth)\"%( t2 ) ) \n",
    "                        l.AddEntry(h_pred_p2    , \"D(%s) (pred)\"%( t2 ) ) \n",
    "\n",
    "                    l.AddEntry(h_truth_pp  , \"D(%s) (truth)\"%( \",\".join(comb) ) ) \n",
    "                    l.AddEntry(h_pred_pp   , \"D(%s) (pred)\"%( \",\".join(comb) ) ) \n",
    "\n",
    "                    l.AddEntry(h_yield     , \"yield\" ) \n",
    "\n",
    "                    lines = [ \n",
    "                            (0.16, 0.965, 'Epoch %5i    Loss %6.4f'%( epoch, loss ))\n",
    "                            ]\n",
    "\n",
    "\n",
    "                    h_yield.Scale(max_/h_yield.GetMaximum())\n",
    "                    for logY in [True, False]:\n",
    "                        c1 = ROOT.TCanvas()\n",
    "                        h_yield   .Draw(\"hist\")\n",
    "                        h_yield   .GetYaxis().SetRangeUser(0.001 if logY else 0, 10**(1.5)*max_ if logY else 1.5*max_)\n",
    "                        h_yield   .Draw(\"hist\")\n",
    "                        h_pred_p1  .Draw(\"hsame\") \n",
    "                        h_truth_p1 .Draw(\"hsame\")\n",
    "                        if t1!=t2: \n",
    "                            h_pred_p2  .Draw(\"hsame\") \n",
    "                            h_truth_p2 .Draw(\"hsame\") \n",
    "                        h_pred_pp .Draw(\"hsame\")\n",
    "                        h_truth_pp.Draw(\"hsame\")\n",
    "                        c1.SetLogy(logY) \n",
    "                        l.Draw()\n",
    "\n",
    "                        drawObjects = [ tex.DrawLatex(*line) for line in lines ]\n",
    "                        for o in drawObjects:\n",
    "                            o.Draw()\n",
    "\n",
    "                        plot_directory_final = os.path.join(plot_directory, \"log\" if logY else \"lin\")\n",
    "                        helpers.copyIndexPHP( plot_directory )\n",
    "                        c1.Print( os.path.join( plot_directory, \"epoch_%05i_%s.png\"%(epoch, var) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9f35dc-1013-4b49-bc35-b5836ba0fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d8a66e-4f90-472b-8587-f631a38f1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b683850-3419-4302-96a9-a2791b59031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_c_1, c_1 in enumerate(coefficients):\n",
    " print(i_c_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211d99cc-12d0-4bb8-a69a-9bcc03c4525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fdbb42-2b6e-4d76-b4d7-b65cdc87fde7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda-ptur]",
   "language": "python",
   "name": "conda-env-conda-ptur-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
