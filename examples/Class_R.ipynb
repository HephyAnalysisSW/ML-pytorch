{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe54142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.24/06\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import ZH_Nakamura\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from Tools import syncer \n",
    "from Tools import user\n",
    "from Tools import helpers\n",
    "import ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26d2ff07-baf1-4bc5-a3a7-7e69d51b5abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weight_ratio(r, weights, base_points):\n",
    "    output=0\n",
    "    nCoefficients=len(r.coefficient_names)\n",
    "    row,column=np.triu_indices(nCoefficients+1)\n",
    "    i, j = np.triu_indices(8)                                                                                       \n",
    "    for i_comb, combination in enumerate(r.combination_list):\n",
    "        output+=torch.outer(torch.from_numpy(weights[r.combination_list[i_comb]]/weights[()]),(base_points[:,row[i_comb]]*base_points[:,column[i_comb]]))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25fe76ec-6005-4120-9a6d-a44df20615bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_loss(r,features,base_points,weights):\n",
    "        fhat = 1./(1. + r.predict_r_hat(features,base_points))\n",
    "        weight_ratios=make_weight_ratio(r, weights, base_points)\n",
    "        loss = (torch.from_numpy(weights[()])*torch.transpose(weight_ratios*fhat**2 + (1-fhat)**2,0,1)).sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea0aab83-ba17-4988-a287-8d8e2aafcc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class R:\n",
    "    def __init__(self,nfeatures,coefficient_names):\n",
    "        self.nfeatures         = nfeatures\n",
    "        self.coefficient_names = coefficient_names\n",
    "        self.combination_list=list(itertools.chain.from_iterable(itertools.combinations_with_replacement(self.coefficient_names, i) for i in np.arange(0,3)))\n",
    "        self.n_hat = {combination: self.make_NN() for combination in self.combination_list}\n",
    "        \n",
    "    def make_NN(self, hidden_layers  = [32, 32, 32, 32]):\n",
    "        model_nn = [torch.nn.BatchNorm1d(self.nfeatures), torch.nn.ReLU(), torch.nn.Linear(self.nfeatures, hidden_layers[0])]\n",
    "        for i_layer, layer in enumerate(hidden_layers):\n",
    "            model_nn.append(torch.nn.Linear(hidden_layers[i_layer], hidden_layers[i_layer+1] if i_layer+1<len(hidden_layers) else 1))\n",
    "            if i_layer+1<len(hidden_layers):\n",
    "                model_nn.append( torch.nn.ReLU() )\n",
    "        return torch.nn.Sequential(*model_nn)\n",
    "\n",
    "    def evaluate_NN(self, features):\n",
    "        '''Evaluate Neural Network: The zeroth dimension of features is the number of data points and and the first dimension\n",
    "        is the number of features(variables)\n",
    "        '''\n",
    "        noutputs=len(self.combination_list)\n",
    "        ndatapoints=features.shape[0]\n",
    "        \n",
    "        output=torch.zeros((noutputs,ndatapoints))\n",
    "        for i in range(noutputs):\n",
    "            #self.n_hat[self.combination_list[i]].eval()\n",
    "            x=self.n_hat[self.combination_list[i]](features)\n",
    "            if i==0:\n",
    "                output[i,:]=1\n",
    "            else:\n",
    "                output[i,:]=torch.flatten(x)            \n",
    "        return output\n",
    "        \n",
    "    \n",
    "    def predict_r_hat(self, features, base_points):\n",
    "        '''Evaluate positive xsec ratio for given theta and \n",
    "        '''\n",
    "        ndatapoints=features.shape[0]\n",
    "        output_NN = self.evaluate_NN(features)\n",
    "        n_terms=len(self.coefficient_names)\n",
    "        row,column=np.triu_indices(n_terms+1)\n",
    "        Omega=torch.zeros((n_terms+1,n_terms+1,ndatapoints))\n",
    "        for i in range(0, len(row)):\n",
    "            Omega[row[i]][column[i]][:]=output_NN[i,:]\n",
    "        Omega_swapped=torch.swapaxes(Omega,1,2)\n",
    "        Omega_swapped=torch.swapaxes(Omega_swapped,0,1)\n",
    "        \n",
    "        out=torch.matmul(Omega_swapped,torch.transpose(base_points,0,1))\n",
    "        return torch.linalg.norm(out, 2, 1)\n",
    "    \n",
    "    def save(self,fileName):\n",
    "        outfile = open(fileName,'wb')\n",
    "        pickle.dump(self, outfile)\n",
    "        outfile.close()\n",
    "        \n",
    "    @classmethod\n",
    "    def load(self, fileName):\n",
    "        infile = open(fileName,'rb')\n",
    "        print(fileName)\n",
    "        new_dict = pickle.load(infile)\n",
    "        infile.close()\n",
    "        return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7f05cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 2)\n"
     ]
    }
   ],
   "source": [
    "n_features=6\n",
    "#coefficients=['cHQ3', 'cHW', 'cHWtil']\n",
    "#base_points = np.asarray([np.array([1, value1, value2, value3]) for value1 in [-1.5, -.8, .2, 0., .2, .8, 1.5]  for value2 in [-1.5, -.8, .2, 0, .2, .8, 1.5] for value3 in [-1.5, -.8, .2, 0, .2, .8, 1.5]])\n",
    "#coefficients=['cHW', 'cHWtil']\n",
    "#base_points = np.asarray([np.array([1, value1, value2]) for value1 in [-1.5, -.8, .2, 0., .2, .8, 1.5]  for value2 in [-1.5, -.8, .2, 0, .2, .8, 1.5]])\n",
    "coefficients=['cHQ3']\n",
    "base_points = np.asarray([np.array([1, value1]) for value1 in [-1.5, -.8, .2, 0., .2, .8, 1.5]])\n",
    "print(base_points.shape)\n",
    "r_NN = R(n_features, coefficients)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d8e3dc7-f2ba-4849-9d50-1bf07fa46b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_directory=\"v4_Class_7\"\n",
    "\n",
    "nEvents=30000\n",
    "learning_rate = 1e-3\n",
    "device        = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_epoch       = 10000\n",
    "plot_every    = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a181dfe7-810d-4711-a6cd-2bdc69654ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested 30000 events. Simulated 30000 events and 30000 survive pT_min cut of 0.\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "import ZH_Nakamura \n",
    "ZH_Nakamura.feature_names = ZH_Nakamura.feature_names[0:6] # restrict features\n",
    "features   = ZH_Nakamura.getEvents(nEvents)[:,0:6]\n",
    "feature_names  = ZH_Nakamura.feature_names\n",
    "plot_options   = ZH_Nakamura.plot_options\n",
    "plot_vars      = ZH_Nakamura.feature_names\n",
    "\n",
    "mask       = (features[:,feature_names.index('pT')]<900) & (features[:,feature_names.index('sqrt_s_hat')]<1800) \n",
    "features = features[mask]\n",
    "\n",
    "n_features = len(features[0])\n",
    "weights    = ZH_Nakamura.getWeights(features, ZH_Nakamura.make_eft())\n",
    "\n",
    "\n",
    "WC='cHQ3'\n",
    "\n",
    "pT=features[:,feature_names.index('pT')]\n",
    "features=torch.from_numpy(features)\n",
    "#w0_train       = torch.from_numpy(weights[()]).float().to(device)\n",
    "#wp_train       = torch.from_numpy(weights[(WC,)]).float().to(device)\n",
    "#wpp_train      = torch.from_numpy(weights[(WC,WC)]).float().to(device)\n",
    "w0_train       = weights[()]\n",
    "wp_train       = weights[(WC,)]\n",
    "wpp_train      = weights[(WC,WC)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42481a4e-fa28-4a93-b413-2d085ac01cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params=[]\n",
    "for comb in r_NN.combination_list:\n",
    "        all_params+=r_NN.n_hat[comb].parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79bce9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(list(all_params), lr=learning_rate)\n",
    "losses = []\n",
    "losses_train=[]\n",
    "for comb in r_NN.combination_list:\n",
    "    r_NN.n_hat[comb].train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae5c4fdd-210e-4204-88ef-d80cc62630f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tex = ROOT.TLatex()\n",
    "tex.SetNDC()\n",
    "tex.SetTextSize(0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0984cc6-b355-4e3e-bec0-3020b124d8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss outside loop 2.1027393494775173\n",
      "epoch 1 loss outside loop 2.0950571602294374\n",
      "epoch 2 loss outside loop 2.0876983784153778\n",
      "epoch 3 loss outside loop 2.0806133279214074\n",
      "epoch 4 loss outside loop 2.0737200547741845\n",
      "epoch 5 loss outside loop 2.066965675490178\n",
      "epoch 6 loss outside loop 2.0602082556285826\n",
      "epoch 7 loss outside loop 2.0533511678653196\n",
      "epoch 8 loss outside loop 2.046320468295894\n",
      "epoch 9 loss outside loop 2.038990097911599\n",
      "epoch 10 loss outside loop 2.031406261730531\n",
      "epoch 11 loss outside loop 2.023530056559137\n",
      "epoch 12 loss outside loop 2.0153325330862892\n",
      "epoch 13 loss outside loop 2.0067652418952533\n",
      "epoch 14 loss outside loop 1.9977615349003544\n",
      "epoch 15 loss outside loop 1.9882523057810815\n",
      "epoch 16 loss outside loop 1.978173004824899\n",
      "epoch 17 loss outside loop 1.967428461639244\n",
      "epoch 18 loss outside loop 1.9559320122389676\n",
      "epoch 19 loss outside loop 1.9435590381176895\n",
      "epoch 20 loss outside loop 1.9301793597570032\n",
      "epoch 21 loss outside loop 1.9156716011044006\n",
      "epoch 22 loss outside loop 1.899916170285382\n",
      "epoch 23 loss outside loop 1.8827968264091757\n",
      "epoch 24 loss outside loop 1.8641629752711046\n",
      "epoch 25 loss outside loop 1.8438690958981416\n",
      "epoch 26 loss outside loop 1.8217976712051616\n",
      "epoch 27 loss outside loop 1.7977674674853235\n",
      "epoch 28 loss outside loop 1.771623678689262\n",
      "epoch 29 loss outside loop 1.7432948792520033\n",
      "epoch 30 loss outside loop 1.712736974835524\n",
      "epoch 31 loss outside loop 1.6799327082198872\n",
      "epoch 32 loss outside loop 1.6449317153429144\n",
      "epoch 33 loss outside loop 1.6078422786404862\n",
      "epoch 34 loss outside loop 1.5687576213316754\n",
      "epoch 35 loss outside loop 1.5278932301465022\n",
      "epoch 36 loss outside loop 1.485520613503723\n",
      "epoch 37 loss outside loop 1.4419872403999188\n",
      "epoch 38 loss outside loop 1.397677708192791\n",
      "epoch 39 loss outside loop 1.3530815452833176\n",
      "epoch 40 loss outside loop 1.308630779158395\n",
      "epoch 41 loss outside loop 1.26482425939684\n",
      "epoch 42 loss outside loop 1.2220814316056567\n",
      "epoch 43 loss outside loop 1.1807624068124505\n",
      "epoch 44 loss outside loop 1.1411842200736224\n",
      "epoch 45 loss outside loop 1.1036333398110956\n",
      "epoch 46 loss outside loop 1.068329839336221\n",
      "epoch 47 loss outside loop 1.0354347227968586\n",
      "epoch 48 loss outside loop 1.0050440794692477\n",
      "epoch 49 loss outside loop 0.9771626290757612\n",
      "epoch 50 loss outside loop 0.9517963349057806\n",
      "epoch 51 loss outside loop 0.9288884454338107\n",
      "epoch 52 loss outside loop 0.9083511484502459\n",
      "epoch 53 loss outside loop 0.8900579577109589\n",
      "epoch 54 loss outside loop 0.8738578405726665\n",
      "epoch 55 loss outside loop 0.8595898007176634\n",
      "epoch 56 loss outside loop 0.8470866361539033\n",
      "epoch 57 loss outside loop 0.8361819363282379\n",
      "epoch 58 loss outside loop 0.8266989509577058\n",
      "epoch 59 loss outside loop 0.8185036627061348\n",
      "epoch 60 loss outside loop 0.8114524175990605\n",
      "epoch 61 loss outside loop 0.8054102757362578\n",
      "epoch 62 loss outside loop 0.8002553766709166\n",
      "epoch 63 loss outside loop 0.7958742848274153\n",
      "epoch 64 loss outside loop 0.7921652965081385\n",
      "epoch 65 loss outside loop 0.7890335316062053\n",
      "epoch 66 loss outside loop 0.7863907383767166\n",
      "epoch 67 loss outside loop 0.7841637411283884\n",
      "epoch 68 loss outside loop 0.7822911238231898\n",
      "epoch 69 loss outside loop 0.7807236498790544\n",
      "epoch 70 loss outside loop 0.7794251095985603\n",
      "epoch 71 loss outside loop 0.778361824228828\n",
      "epoch 72 loss outside loop 0.7774928938505962\n",
      "epoch 73 loss outside loop 0.7767905727236017\n",
      "epoch 74 loss outside loop 0.7762260414849977\n",
      "epoch 75 loss outside loop 0.7757739393078276\n",
      "epoch 76 loss outside loop 0.7754114080581049\n",
      "epoch 77 loss outside loop 0.775118972344427\n",
      "epoch 78 loss outside loop 0.774879447961919\n",
      "epoch 79 loss outside loop 0.7746786329760681\n",
      "epoch 80 loss outside loop 0.7745044216885749\n",
      "epoch 81 loss outside loop 0.7743477283785041\n",
      "epoch 82 loss outside loop 0.7742007832684872\n",
      "epoch 83 loss outside loop 0.7740580361208884\n",
      "epoch 84 loss outside loop 0.7739146928258746\n",
      "epoch 85 loss outside loop 0.7737676781609131\n",
      "epoch 86 loss outside loop 0.7736146061885654\n",
      "epoch 87 loss outside loop 0.7734541050184175\n",
      "epoch 88 loss outside loop 0.7732854337177064\n",
      "epoch 89 loss outside loop 0.7731086749390159\n",
      "epoch 90 loss outside loop 0.7729241246338909\n",
      "epoch 91 loss outside loop 0.7727322030593907\n",
      "epoch 92 loss outside loop 0.7725336009860156\n",
      "epoch 93 loss outside loop 0.7723292556007414\n",
      "epoch 94 loss outside loop 0.7721200882280843\n",
      "epoch 95 loss outside loop 0.7719071259430088\n",
      "epoch 96 loss outside loop 0.7716913832550707\n",
      "epoch 97 loss outside loop 0.7714738528188094\n",
      "epoch 98 loss outside loop 0.7712555095622922\n",
      "epoch 99 loss outside loop 0.7710371876188964\n",
      "epoch 100 loss outside loop 0.7708196218619161\n",
      "epoch 101 loss outside loop 0.7706034419428294\n",
      "epoch 102 loss outside loop 0.7703891651462413\n",
      "epoch 103 loss outside loop 0.7701771607817341\n",
      "epoch 104 loss outside loop 0.7699677225497581\n",
      "epoch 105 loss outside loop 0.7697610189729949\n",
      "epoch 106 loss outside loop 0.7695570903933309\n",
      "epoch 107 loss outside loop 0.7693559565881337\n",
      "epoch 108 loss outside loop 0.7691574799044614\n",
      "epoch 109 loss outside loop 0.768961416982108\n",
      "epoch 110 loss outside loop 0.7687674776927874\n",
      "epoch 111 loss outside loop 0.7685753892025899\n",
      "epoch 112 loss outside loop 0.7683847936579802\n",
      "epoch 113 loss outside loop 0.7681953234709613\n",
      "epoch 114 loss outside loop 0.7680066069454982\n",
      "epoch 115 loss outside loop 0.7678182395277116\n",
      "epoch 116 loss outside loop 0.7676298564990915\n",
      "epoch 117 loss outside loop 0.7674411752489195\n",
      "epoch 118 loss outside loop 0.7672518238228465\n",
      "epoch 119 loss outside loop 0.7670615465451629\n",
      "epoch 120 loss outside loop 0.7668701392334769\n",
      "epoch 121 loss outside loop 0.7666773431579756\n",
      "epoch 122 loss outside loop 0.7664829884604893\n",
      "epoch 123 loss outside loop 0.766286936284029\n",
      "epoch 124 loss outside loop 0.766089044957146\n",
      "epoch 125 loss outside loop 0.7658891126680452\n",
      "epoch 126 loss outside loop 0.7656871845433038\n",
      "epoch 127 loss outside loop 0.7654831798150648\n",
      "epoch 128 loss outside loop 0.7652769639558221\n",
      "epoch 129 loss outside loop 0.7650685549127785\n",
      "epoch 130 loss outside loop 0.7648581372908557\n",
      "epoch 131 loss outside loop 0.7646455830169322\n",
      "epoch 132 loss outside loop 0.7644308447103307\n",
      "epoch 133 loss outside loop 0.7642139018619012\n",
      "epoch 134 loss outside loop 0.7639946243912817\n",
      "epoch 135 loss outside loop 0.7637728900157643\n",
      "epoch 136 loss outside loop 0.7635486748790699\n",
      "epoch 137 loss outside loop 0.7633219533375342\n",
      "epoch 138 loss outside loop 0.7630926288647404\n",
      "epoch 139 loss outside loop 0.762860726590041\n",
      "epoch 140 loss outside loop 0.7626262595028066\n",
      "epoch 141 loss outside loop 0.7623889795022667\n",
      "epoch 142 loss outside loop 0.7621489718920698\n",
      "epoch 143 loss outside loop 0.7619064035824108\n",
      "epoch 144 loss outside loop 0.7616613654746935\n",
      "epoch 145 loss outside loop 0.7614136813819348\n",
      "epoch 146 loss outside loop 0.7611632858613646\n",
      "epoch 147 loss outside loop 0.7609101247313702\n",
      "epoch 148 loss outside loop 0.760654111252133\n",
      "epoch 149 loss outside loop 0.760395113633082\n",
      "epoch 150 loss outside loop 0.7601332504394768\n",
      "epoch 151 loss outside loop 0.7598682763883923\n",
      "epoch 152 loss outside loop 0.7596002972670928\n",
      "epoch 153 loss outside loop 0.7593293895855409\n",
      "epoch 154 loss outside loop 0.7590552745678012\n",
      "epoch 155 loss outside loop 0.7587782700139544\n",
      "epoch 156 loss outside loop 0.7584981925046038\n",
      "epoch 157 loss outside loop 0.7582150932501751\n",
      "epoch 158 loss outside loop 0.7579287674847741\n",
      "epoch 159 loss outside loop 0.7576393678668984\n",
      "epoch 160 loss outside loop 0.7573468118795355\n",
      "epoch 161 loss outside loop 0.7570509086380207\n",
      "epoch 162 loss outside loop 0.7567519795550891\n",
      "epoch 163 loss outside loop 0.7564500198691207\n",
      "epoch 164 loss outside loop 0.7561449146129391\n",
      "epoch 165 loss outside loop 0.7558368381036429\n",
      "epoch 166 loss outside loop 0.7555261574349694\n",
      "epoch 167 loss outside loop 0.7552132404917348\n",
      "epoch 168 loss outside loop 0.7548979331772847\n",
      "epoch 169 loss outside loop 0.7545808809294883\n",
      "epoch 170 loss outside loop 0.7542621194741232\n",
      "epoch 171 loss outside loop 0.753943205071885\n",
      "epoch 172 loss outside loop 0.7536246668256114\n",
      "epoch 173 loss outside loop 0.7533085198224145\n",
      "epoch 174 loss outside loop 0.7529954278031206\n",
      "epoch 175 loss outside loop 0.7526841523930652\n",
      "epoch 176 loss outside loop 0.7523742505653482\n",
      "epoch 177 loss outside loop 0.7520671962710699\n",
      "epoch 178 loss outside loop 0.7517628018462997\n",
      "epoch 179 loss outside loop 0.7514629134888691\n",
      "epoch 180 loss outside loop 0.7511658778009932\n",
      "epoch 181 loss outside loop 0.7508724389009239\n",
      "epoch 182 loss outside loop 0.7505829029739993\n",
      "epoch 183 loss outside loop 0.750298336697851\n",
      "epoch 184 loss outside loop 0.7500176915159513\n",
      "epoch 185 loss outside loop 0.7497415678517401\n",
      "epoch 186 loss outside loop 0.749470097047207\n",
      "epoch 187 loss outside loop 0.7492027056104484\n",
      "epoch 188 loss outside loop 0.7489393948938461\n",
      "epoch 189 loss outside loop 0.7486806999586147\n",
      "epoch 190 loss outside loop 0.748426946308684\n",
      "epoch 191 loss outside loop 0.7481787964765758\n",
      "epoch 192 loss outside loop 0.7479356801533592\n",
      "epoch 193 loss outside loop 0.7476975716639522\n",
      "epoch 194 loss outside loop 0.7474644689704311\n",
      "epoch 195 loss outside loop 0.74723604213061\n",
      "epoch 196 loss outside loop 0.7470125610928229\n",
      "epoch 197 loss outside loop 0.7467940380814824\n",
      "epoch 198 loss outside loop 0.7465808021910565\n",
      "epoch 199 loss outside loop 0.7463722108422715\n",
      "epoch 200 loss outside loop 0.7461677063094649\n",
      "epoch 201 loss outside loop 0.7459672977736841\n",
      "epoch 202 loss outside loop 0.7457714572200671\n",
      "epoch 203 loss outside loop 0.7455803947346884\n",
      "epoch 204 loss outside loop 0.7453940365982088\n",
      "epoch 205 loss outside loop 0.7452115468352216\n",
      "epoch 206 loss outside loop 0.7450331301250203\n",
      "epoch 207 loss outside loop 0.7448586009315847\n",
      "epoch 208 loss outside loop 0.7446880051186826\n",
      "epoch 209 loss outside loop 0.7445211775553764\n",
      "epoch 210 loss outside loop 0.7443575997648249\n",
      "epoch 211 loss outside loop 0.7441976059533066\n",
      "epoch 212 loss outside loop 0.7440408015632775\n",
      "epoch 213 loss outside loop 0.7438870769116906\n",
      "epoch 214 loss outside loop 0.7437364780246732\n",
      "epoch 215 loss outside loop 0.7435893950210712\n",
      "epoch 216 loss outside loop 0.743444710236494\n",
      "epoch 217 loss outside loop 0.743302056974368\n",
      "epoch 218 loss outside loop 0.743161354974389\n",
      "epoch 219 loss outside loop 0.7430225826859522\n",
      "epoch 220 loss outside loop 0.7428857934673024\n",
      "epoch 221 loss outside loop 0.7427507939284543\n",
      "epoch 222 loss outside loop 0.7426168100282762\n",
      "epoch 223 loss outside loop 0.7424837367125814\n",
      "epoch 224 loss outside loop 0.7423508502728\n",
      "epoch 225 loss outside loop 0.742218001162001\n",
      "epoch 226 loss outside loop 0.7420854589720942\n",
      "epoch 227 loss outside loop 0.7419533147521298\n",
      "epoch 228 loss outside loop 0.7418214635797924\n",
      "epoch 229 loss outside loop 0.7416899941467727\n",
      "epoch 230 loss outside loop 0.7415593856932048\n",
      "epoch 231 loss outside loop 0.7414295824555264\n",
      "epoch 232 loss outside loop 0.7412997844468003\n",
      "epoch 233 loss outside loop 0.7411707655887416\n",
      "epoch 234 loss outside loop 0.7410417320915287\n",
      "epoch 235 loss outside loop 0.7409122596185138\n",
      "epoch 236 loss outside loop 0.7407822742157127\n",
      "epoch 237 loss outside loop 0.7406517388963958\n",
      "epoch 238 loss outside loop 0.7405207502215365\n",
      "epoch 239 loss outside loop 0.7403894947117475\n",
      "epoch 240 loss outside loop 0.7402579405107628\n",
      "epoch 241 loss outside loop 0.7401261579861699\n",
      "epoch 242 loss outside loop 0.7399944017521982\n",
      "epoch 243 loss outside loop 0.7398623899115798\n",
      "epoch 244 loss outside loop 0.7397304836701983\n",
      "epoch 245 loss outside loop 0.7395985980842201\n",
      "epoch 246 loss outside loop 0.739466800451102\n",
      "epoch 247 loss outside loop 0.739334969922219\n",
      "epoch 248 loss outside loop 0.7392030621602722\n",
      "epoch 249 loss outside loop 0.739070673019155\n",
      "epoch 250 loss outside loop 0.7389376326761044\n",
      "epoch 251 loss outside loop 0.7388042844739955\n",
      "epoch 252 loss outside loop 0.7386710271011696\n",
      "epoch 253 loss outside loop 0.7385378402081688\n",
      "epoch 254 loss outside loop 0.7384043603729362\n",
      "epoch 255 loss outside loop 0.7382704445215242\n",
      "epoch 256 loss outside loop 0.7381360708006264\n",
      "epoch 257 loss outside loop 0.7380014258434271\n",
      "epoch 258 loss outside loop 0.7378663787482695\n",
      "epoch 259 loss outside loop 0.7377310628153446\n",
      "epoch 260 loss outside loop 0.737595902249705\n",
      "epoch 261 loss outside loop 0.7374605786042272\n",
      "epoch 262 loss outside loop 0.7373249476945425\n",
      "epoch 263 loss outside loop 0.737189016013436\n",
      "epoch 264 loss outside loop 0.7370528059244552\n",
      "epoch 265 loss outside loop 0.7369160701887254\n",
      "epoch 266 loss outside loop 0.7367791211691206\n",
      "epoch 267 loss outside loop 0.7366420497034036\n",
      "epoch 268 loss outside loop 0.7365049665456587\n",
      "epoch 269 loss outside loop 0.736367538683169\n",
      "epoch 270 loss outside loop 0.736229713546291\n",
      "epoch 271 loss outside loop 0.7360920905263911\n",
      "epoch 272 loss outside loop 0.7359541313868481\n",
      "epoch 273 loss outside loop 0.7358158522390226\n",
      "epoch 274 loss outside loop 0.7356767901592653\n",
      "epoch 275 loss outside loop 0.7355372301276807\n",
      "epoch 276 loss outside loop 0.735396853791477\n",
      "epoch 277 loss outside loop 0.7352559249270779\n",
      "epoch 278 loss outside loop 0.7351148262912623\n",
      "epoch 279 loss outside loop 0.7349735374982516\n",
      "epoch 280 loss outside loop 0.7348321377805969\n",
      "epoch 281 loss outside loop 0.7346908418409708\n",
      "epoch 282 loss outside loop 0.7345492587590687\n",
      "epoch 283 loss outside loop 0.7344078904237232\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epoch):\n\u001b[0;32m----> 2\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mf_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr_NN\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_points\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss outside loop\u001b[39m\u001b[38;5;124m\"\u001b[39m,  loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m      5\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mf_loss\u001b[0;34m(r, features, base_points, weights)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf_loss\u001b[39m(r,features,base_points,weights):\n\u001b[0;32m----> 2\u001b[0m         fhat \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_r_hat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbase_points\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m         weight_ratios\u001b[38;5;241m=\u001b[39mmake_weight_ratio(r, weights, base_points)\n\u001b[1;32m      4\u001b[0m         loss \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mfrom_numpy(weights[()])\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtranspose(weight_ratios\u001b[38;5;241m*\u001b[39mfhat\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mfhat)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msum()\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mR.predict_r_hat\u001b[0;34m(self, features, base_points)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m'''Evaluate positive xsec ratio for given theta and \u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     37\u001b[0m ndatapoints\u001b[38;5;241m=\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 38\u001b[0m output_NN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_NN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m n_terms\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoefficient_names)\n\u001b[1;32m     40\u001b[0m row,column\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mtriu_indices(n_terms\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mR.evaluate_NN\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     23\u001b[0m output\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mzeros((noutputs,ndatapoints))\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(noutputs):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m#self.n_hat[self.combination_list[i]].eval()\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombination_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     28\u001b[0m         output[i,:]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/groups/hephy/cms/robert.schoefbeck/conda/envs/ptur/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/groups/hephy/cms/robert.schoefbeck/conda/envs/ptur/lib/python3.10/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/groups/hephy/cms/robert.schoefbeck/conda/envs/ptur/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/groups/hephy/cms/robert.schoefbeck/conda/envs/ptur/lib/python3.10/site-packages/torch/nn/modules/activation.py:98\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/groups/hephy/cms/robert.schoefbeck/conda/envs/ptur/lib/python3.10/site-packages/torch/nn/functional.py:1442\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1442\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epoch):\n",
    "    loss = f_loss(r_NN,features,torch.from_numpy(base_points),weights)\n",
    "    print(\"epoch\", epoch, \"loss outside loop\",  loss.item())\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    optimizer.zero_grad()    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    '''if (epoch % plot_every)==0:\n",
    "        with torch.no_grad():\n",
    "            pred_p = r_NN.n_hat[('cHQ3',)](features).squeeze().cpu().detach().numpy()\n",
    "            pred_pp = r_NN.n_hat[('cHQ3', 'cHQ3')](features).squeeze().cpu().detach().numpy()\n",
    "            predicted_weights={():w0_train, ('cHQ3',):pred_p, ('cHQ3', 'cHQ3'):pred_pp}\n",
    "            \n",
    "            loss_train = f_loss(r_NN, features, torch.from_numpy(base_points), predicted_weights)\n",
    "            losses_train.append(loss_train.item())\n",
    "            \n",
    "            print(\"epoch\", epoch, \"loss inside loop\",  loss_train.item())\n",
    "                       \n",
    "            for var in plot_vars:\n",
    "                binning     = plot_options[var]['binning']\n",
    "                np_binning  = np.linspace(binning[1], binning[2], 1+binning[0])\n",
    "\n",
    "                \n",
    "                truth_0  = np.histogram(features[:,feature_names.index(var)], np_binning, weights=w0_train )\n",
    "                truth_p  = np.histogram(features[:,feature_names.index(var)], np_binning, weights=wp_train )\n",
    "                truth_pp = np.histogram(features[:,feature_names.index(var)], np_binning, weights=wpp_train )\n",
    "\n",
    "                pred_0_hist  = np.histogram(features[:,feature_names.index(var)], np_binning, weights=w0_train)\n",
    "                pred_p_hist  = np.histogram(features[:,feature_names.index(var)], np_binning, weights=w0_train*pred_p)\n",
    "                pred_pp_hist = np.histogram(features[:,feature_names.index(var)], np_binning, weights=w0_train*pred_pp)\n",
    "\n",
    "                h_yield       = helpers.make_TH1F(truth_0)\n",
    "                h_truth_p     = helpers.make_TH1F(truth_p)\n",
    "                h_truth_p     .Divide(h_yield) \n",
    "                h_truth_pp    = helpers.make_TH1F(truth_pp)\n",
    "                h_truth_pp    .Divide(h_yield) \n",
    "\n",
    "                h_pred_p      = helpers.make_TH1F(pred_p_hist)\n",
    "                h_pred_p      .Divide(h_yield) \n",
    "                h_pred_pp     = helpers.make_TH1F(pred_pp_hist)\n",
    "                h_pred_pp     .Divide(h_yield) \n",
    "\n",
    "                l = ROOT.TLegend(0.3,0.7,0.9,0.95)\n",
    "                l.SetNColumns(2)\n",
    "                l.SetFillStyle(0)\n",
    "                l.SetShadowColor(ROOT.kWhite)\n",
    "                l.SetBorderSize(0)\n",
    "\n",
    "                h_yield      .SetLineColor(ROOT.kGray+2) \n",
    "                h_truth_p    .SetLineColor(ROOT.kBlue) \n",
    "                h_truth_pp   .SetLineColor(ROOT.kRed) \n",
    "                h_pred_p     .SetLineColor(ROOT.kBlue) \n",
    "                h_pred_pp    .SetLineColor(ROOT.kRed) \n",
    "                h_yield      .SetMarkerColor(ROOT.kGray+2) \n",
    "                h_truth_p    .SetMarkerColor(ROOT.kBlue) \n",
    "                h_truth_pp   .SetMarkerColor(ROOT.kRed) \n",
    "                h_pred_p     .SetMarkerColor(ROOT.kBlue) \n",
    "                h_pred_pp    .SetMarkerColor(ROOT.kRed) \n",
    "                h_yield      .SetMarkerStyle(0)\n",
    "                h_truth_p    .SetMarkerStyle(0)\n",
    "                h_truth_pp   .SetMarkerStyle(0)\n",
    "                h_pred_p     .SetMarkerStyle(0)\n",
    "                h_pred_pp    .SetMarkerStyle(0)\n",
    "\n",
    "                l.AddEntry(h_truth_p   , \"1^{st.} der (truth)\" ) \n",
    "                l.AddEntry(h_truth_pp  , \"2^{st.} der (truth)\" ) \n",
    "                l.AddEntry(h_pred_p    , \"1^{st.} der (pred)\" ) \n",
    "                l.AddEntry(h_pred_pp   , \"2^{st.} der (pred)\" ) \n",
    "                l.AddEntry(h_yield     , \"yield\" ) \n",
    "\n",
    "                h_truth_p    .SetLineStyle(ROOT.kDashed) \n",
    "                h_truth_pp   .SetLineStyle(ROOT.kDashed)\n",
    "\n",
    "                lines = [ \n",
    "                        (0.16, 0.965, 'Epoch %5i    Loss %6.4f'%( epoch, loss ))\n",
    "                        ]\n",
    "\n",
    "                max_ = max( map( lambda h:h.GetMaximum(), [ h_truth_p, h_truth_pp ] ))\n",
    "\n",
    "                h_yield.Scale(max_/h_yield.GetMaximum())\n",
    "                for logY in [True, False]:\n",
    "                    c1 = ROOT.TCanvas()\n",
    "                    h_yield   .Draw(\"hist\")\n",
    "                    h_yield   .GetYaxis().SetRangeUser(0.1 if logY else 0, 10**(1.5)*max_ if logY else 1.5*max_)\n",
    "                    h_yield   .Draw(\"hist\")\n",
    "                    h_truth_p .Draw(\"hsame\") \n",
    "                    h_truth_pp.Draw(\"hsame\")\n",
    "                    h_pred_p  .Draw(\"hsame\") \n",
    "                    h_pred_pp .Draw(\"hsame\")\n",
    "                    c1.SetLogy(logY) \n",
    "                    l.Draw()\n",
    "\n",
    "                    drawObjects = [ tex.DrawLatex(*line) for line in lines ]\n",
    "                    for o in drawObjects:\n",
    "                        o.Draw()\n",
    "\n",
    "                    plot_directory_final = os.path.join(plot_directory, \"log\" if logY else \"lin\")\n",
    "                    helpers.copyIndexPHP( plot_directory )\n",
    "                    c1.Print( os.path.join( plot_directory, \"epoch_%05i_%s.png\"%(epoch, var) ) )\n",
    "                    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65483d66-686f-4680-8930-8a97698e7d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda-ptur]",
   "language": "python",
   "name": "conda-env-conda-ptur-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
