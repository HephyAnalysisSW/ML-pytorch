{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0fe54142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import ZH_Nakamura\n",
    "from Class_R import R\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "25fe76ec-6005-4120-9a6d-a44df20615bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_loss(r,features,base_points):\n",
    "        loss=0\n",
    "        for i_base_point, base_point in enumerate(base_points):\n",
    "            fhat  = torch.tensor(1./(1. + r.predict_r_hat(features,base_point)), requires_grad=True)\n",
    "            loss += (fhat**2 + (1-fhat)**2).sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea0aab83-ba17-4988-a287-8d8e2aafcc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class R:\n",
    "    def __init__(self,nfeatures,coefficient_names):\n",
    "        self.nfeatures         = nfeatures\n",
    "        self.coefficient_names = coefficient_names\n",
    "        self.combination_list=list(itertools.chain.from_iterable(itertools.combinations_with_replacement(self.coefficient_names, i) for i in np.arange(0,3)))\n",
    "        self.n_hat = {combination:self.make_NN() for combination in self.combination_list}\n",
    "        \n",
    "    def make_NN(self, hidden_layers  = [32, 32, 32, 32]):\n",
    "        model_nn = [torch.nn.BatchNorm1d(self.nfeatures), torch.nn.ReLU(), torch.nn.Linear(self.nfeatures, hidden_layers[0])]\n",
    "        for i_layer, layer in enumerate(hidden_layers):\n",
    "            model_nn.append(torch.nn.Linear(hidden_layers[i_layer], hidden_layers[i_layer+1] if i_layer+1<len(hidden_layers) else 1))\n",
    "            if i_layer+1<len(hidden_layers):\n",
    "                model_nn.append( torch.nn.ReLU() )\n",
    "        return torch.nn.Sequential(*model_nn)\n",
    "\n",
    "    def evaluate_NN(self, features):\n",
    "        '''Evaluate Neural Network: The zeroth dimension of features is the number of data points and and the first dimension\n",
    "        is the number of features(variables)\n",
    "        '''\n",
    "        noutputs=len(self.combination_list)\n",
    "        ndatapoints=features.shape[0]\n",
    "        output=np.zeros((noutputs,ndatapoints))\n",
    "        #print(\"Output Shape:\")\n",
    "        #print(output.shape)\n",
    "        #print(\"Features Shape:\")\n",
    "        #print(features.shape)\n",
    "            \n",
    "        for i in range(noutputs):\n",
    "            x=self.n_hat[self.combination_list[i]](features).detach().numpy()\n",
    "            if i==0:\n",
    "                output[i,:]=1\n",
    "            else:\n",
    "                output[i,:]=x.flatten()            \n",
    "            \n",
    "       # print(\"Output is:\")\n",
    "        #print(output)\n",
    "        return output\n",
    "        \n",
    "    \n",
    "    def predict_r_hat(self, features, theta):\n",
    "        '''Evaluate positive xsec ratio for given theta and \n",
    "        '''\n",
    "        ndatapoints=features.shape[0]\n",
    "        output_NN = self.evaluate_NN(features)\n",
    "        #print(\"Output_NN shape\")\n",
    "        #print(output_NN.shape)\n",
    "        n_terms=len(self.coefficient_names)\n",
    "        #print('N terms: '+str(n_terms))\n",
    "        row,column=np.triu_indices(n_terms)\n",
    "        Omega=np.zeros((n_terms+1,n_terms+1,ndatapoints))\n",
    "        #print(\"Length of Row\")\n",
    "        #print(len(row))\n",
    "        #print(\"Shape of Omega\")\n",
    "        #print(Omega.shape)\n",
    "        for i in range(len(row)):\n",
    "            Omega[row[i]][column[i]][:]=output_NN[i,:]\n",
    "        Omega_swapped=np.swapaxes(Omega,1,2)\n",
    "        Omega_swapped=np.swapaxes(Omega_swapped,0,1)\n",
    "        \n",
    "        #print(\"Shape of Omega swapped\")\n",
    "        #print(Omega_swapped.shape)\n",
    "        #print(\"Shape of theta\")\n",
    "        #print(theta.shape)\n",
    "        \n",
    "        out=np.matmul(Omega_swapped, theta)        \n",
    "        #print(\"Out Shape\")\n",
    "        #print(out.shape)\n",
    "        return torch.tensor(np.linalg.norm(out, 2, 1)**2, requires_grad=True)\n",
    "    \n",
    "    def save(self,fileName):\n",
    "        outfile = open(fileName,'wb')\n",
    "        pickle.dump(self, outfile)\n",
    "        outfile.close()\n",
    "        \n",
    "    @classmethod\n",
    "    def load(self, fileName):\n",
    "        infile = open(fileName,'rb')\n",
    "        print(fileName)\n",
    "        new_dict = pickle.load(infile)\n",
    "        infile.close()\n",
    "        return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e7f05cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=6\n",
    "coefficients=['cHW', 'cHWtil', 'cHQ3']\n",
    "base_points = [np.array([1, value1, value2, value3]) for value1 in [-1.5, -.8, .2, 0., .2, .8, 1.5]  for value2 in [-1.5, -.8, .2, 0, .2, .8, 1.5] for value3 in [-1.5, -.8, .2, 0, .2, .8, 1.5]]\n",
    "n_datapoints=2\n",
    "r = R(n_features, coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9d8e3dc7-f2ba-4849-9d50-1bf07fa46b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_directory=\"v1_with_Biasing\"\n",
    "\n",
    "nEvents=100\n",
    "learning_rate = 1e-3\n",
    "device        = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_epoch       = 10\n",
    "plot_every    = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a181dfe7-810d-4711-a6cd-2bdc69654ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested 100 events. Simulated 100 events and 100 survive pT_min cut of 0.\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "import ZH_Nakamura \n",
    "ZH_Nakamura.feature_names = ZH_Nakamura.feature_names[0:6] # restrict features\n",
    "features   = ZH_Nakamura.getEvents(nEvents)[:,0:6]\n",
    "feature_names  = ZH_Nakamura.feature_names\n",
    "plot_options   = ZH_Nakamura.plot_options\n",
    "plot_vars      = ZH_Nakamura.feature_names\n",
    "\n",
    "mask       = (features[:,feature_names.index('pT')]<900) & (features[:,feature_names.index('sqrt_s_hat')]<1800) \n",
    "features = features[mask]\n",
    "\n",
    "n_features = len(features[0]) \n",
    "weights    = ZH_Nakamura.getWeights(features, ZH_Nakamura.make_eft())\n",
    "\n",
    "\n",
    "pT=features[:,feature_names.index('pT')]\n",
    "features=torch.from_numpy(features)\n",
    "    \n",
    "#for key,value in weights.items():\n",
    " #   value*=bias_factor\n",
    "  #  weights[key]=value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "42481a4e-fa28-4a93-b413-2d085ac01cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params=[]\n",
    "for comb in r.combination_list:\n",
    "    all_params+=r.n_hat[comb].parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "79bce9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(list(all_params), lr=learning_rate)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f0984cc6-b355-4e3e-bec0-3020b124d8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:\n",
      "tensor(10280.3270, grad_fn=<AddBackward0>)\n",
      "Loss:\n",
      "tensor(10280.3270, grad_fn=<AddBackward0>)\n",
      "Loss:\n",
      "tensor(10280.3270, grad_fn=<AddBackward0>)\n",
      "Loss:\n",
      "tensor(10280.3270, grad_fn=<AddBackward0>)\n",
      "Loss:\n",
      "tensor(10280.3270, grad_fn=<AddBackward0>)\n",
      "Loss:\n",
      "tensor(10280.3270, grad_fn=<AddBackward0>)\n",
      "Loss:\n",
      "tensor(10280.3270, grad_fn=<AddBackward0>)\n",
      "Loss:\n",
      "tensor(10280.3270, grad_fn=<AddBackward0>)\n",
      "Loss:\n",
      "tensor(10280.3270, grad_fn=<AddBackward0>)\n",
      "Loss:\n",
      "tensor(10280.3270, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epoch):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    predictions = {combination:r.n_hat[combination](features).squeeze() for combination in r.combination_list}\n",
    "\n",
    "    # Compute and print loss.\n",
    "    \n",
    "    #loss = torch.tensor(f_loss(r,features,base_points), requires_grad=True)\n",
    "    loss = f_loss(r,features,base_points)\n",
    "    loss.requires_grad_(True)\n",
    "    print(\"Loss:\")\n",
    "    print(loss)\n",
    "    losses.append(loss.item())\n",
    "    #if epoch % 100 == 99:\n",
    "    #    print(\"epoch\", epoch, \"loss\",  loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aed5a7-6a12-406d-8988-9053778987ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda-ptur]",
   "language": "python",
   "name": "conda-env-conda-ptur-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
